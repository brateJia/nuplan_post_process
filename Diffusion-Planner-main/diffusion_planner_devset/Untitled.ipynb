{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1951bdae-8ee1-473d-8d4a-8fce9b034817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\anaconda\\envs\\nuplan_py38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "D:\\Anaconda\\anaconda\\envs\\nuplan_py38\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nuplan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 导入项目内的核心模块（确保路径正确）\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusion_planner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiffusion_planner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Diffusion_Planner\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusion_planner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_process\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataProcessor\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusion_planner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDiffusionPlannerInfer\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\CUP\\Foton_intership\\nuplan_project\\Diffusion-Planner-main\\diffusion_planner\\data_process\\data_processor.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnuplan\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactor_state\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate_representation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Point2D\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusion_planner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_process\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroadblock_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m route_roadblock_correction\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusion_planner\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_process\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_process\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m agent_past_process, \n\u001b[0;32m      9\u001b[0m sampled_tracked_objects_to_array_list,\n\u001b[0;32m     10\u001b[0m sampled_static_objects_to_array_list,\n\u001b[0;32m     11\u001b[0m agent_future_process\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nuplan'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict\n",
    "import time  # 直接使用，无需安装\n",
    "\n",
    "# 导入项目内的核心模块（确保路径正确）\n",
    "from diffusion_planner.model.diffusion_planner import Diffusion_Planner\n",
    "from diffusion_planner.data_process.data_processor import DataProcessor\n",
    "from diffusion_planner.utils.config import Config\n",
    "\n",
    "\n",
    "class DiffusionPlannerInfer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_path: str,\n",
    "        ckpt_path: str,\n",
    "        future_params: Dict = {\"time_horizon\": 6.0, \"num_poses\": 20},\n",
    "        enable_ema: bool = True,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        # 加载配置（使用项目内的Config类）\n",
    "        self.config = Config.from_file(config_path)\n",
    "        # 初始化模型和数据处理器（项目内原生实现）\n",
    "        self.model = Diffusion_Planner(self.config)\n",
    "        self.data_processor = DataProcessor(self.config)\n",
    "        self.observation_normalizer = self.config.observation_normalizer\n",
    "        self.device = device\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.enable_ema = enable_ema\n",
    "        self.future_horizon = future_params[\"time_horizon\"]\n",
    "        self.step_interval = self.future_horizon / future_params[\"num_poses\"]\n",
    "\n",
    "        # 加载模型权重\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        state_dict = torch.load(self.ckpt_path, map_location=self.device)\n",
    "        # 处理EMA权重或模型权重层级（项目内训练逻辑）\n",
    "        if self.enable_ema and \"ema_state_dict\" in state_dict:\n",
    "            state_dict = state_dict[\"ema_state_dict\"]\n",
    "        elif \"model\" in state_dict:\n",
    "            state_dict = state_dict[\"model\"]\n",
    "        # 处理多卡训练的\"module.\"前缀\n",
    "        model_state_dict = {k[len(\"module.\"):]: v for k, v in state_dict.items() if k.startswith(\"module.\")}\n",
    "        self.model.load_state_dict(model_state_dict)\n",
    "        self.model.eval()\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def _process_npz(self, npz_path: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"加载npz数据并转换为模型输入（使用项目内DataProcessor的逻辑）\"\"\"\n",
    "        data = np.load(npz_path)\n",
    "        # 调用项目内DataProcessor的observation_adapter（需确保其不依赖nuPlan）\n",
    "        model_inputs = self.data_processor.observation_adapter(\n",
    "            history=data[\"history\"],  # 替换为你npz中实际的历史轨迹键名\n",
    "            traffic_light_data=data[\"traffic_light\"],  # 替换为交通灯数据键名\n",
    "            map_api=None,  # 推理时无nuPlan地图，设为None\n",
    "            route_roadblock_ids=None,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    def _predict_to_trajectory(self, predictions: np.ndarray, ego_init: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"将模型输出转换为轨迹（无nuPlan依赖的原生逻辑）\"\"\"\n",
    "        # 原项目内的轨迹转换逻辑（保留航向角计算）\n",
    "        heading = np.arctan2(predictions[:, 3], predictions[:, 2])[..., None]\n",
    "        pred_trajectory = np.concatenate([predictions[..., :2], heading], axis=-1)\n",
    "        \n",
    "        # 若有初始ego状态，转换为绝对坐标（可选）\n",
    "        if ego_init is not None:\n",
    "            x0, y0, h0 = ego_init\n",
    "            cos_h0, sin_h0 = np.cos(h0), np.sin(h0)\n",
    "            rel_x, rel_y = pred_trajectory[:, 0], pred_trajectory[:, 1]\n",
    "            abs_x = x0 + rel_x * cos_h0 - rel_y * sin_h0\n",
    "            abs_y = y0 + rel_x * sin_h0 + rel_y * cos_h0\n",
    "            abs_heading = h0 + pred_trajectory[:, 2]\n",
    "            pred_trajectory = np.stack([abs_x, abs_y, abs_heading], axis=-1)\n",
    "        \n",
    "        return pred_trajectory\n",
    "\n",
    "    def infer_single(self, npz_path: str) -> np.ndarray:\n",
    "        \"\"\"对单个npz文件执行推理\"\"\"\n",
    "        model_inputs = self._process_npz(npz_path)\n",
    "        model_inputs = self.observation_normalizer(model_inputs)\n",
    "        with torch.no_grad():\n",
    "            _, outputs = self.model(model_inputs)\n",
    "        predictions = outputs[\"prediction\"][0, 0].detach().cpu().numpy().astype(np.float64)\n",
    "        ego_init = np.load(npz_path).get(\"initial_ego_state\", None)\n",
    "        return self._predict_to_trajectory(predictions, ego_init)\n",
    "\n",
    "    def infer_batch(self, data_dir: str) -> None:\n",
    "        \"\"\"遍历文件夹中所有npz文件执行推理\"\"\"\n",
    "        for filename in os.listdir(data_dir):\n",
    "            if filename.endswith(\".npz\"):\n",
    "                npz_path = os.path.join(data_dir, filename)\n",
    "                try:\n",
    "                    trajectory = self.infer_single(npz_path)\n",
    "                    print(f\"✅ 成功推理 {filename}，轨迹形状：{trajectory.shape}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ 推理 {filename} 失败：{str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ====================== 配置参数（与项目路径完全匹配） ======================\n",
    "    CONFIG_PATH = \"nuplan_train.json\"  # 项目内的训练配置文件\n",
    "    CKPT_PATH = \"model.pth\"           # 模型权重路径（需放在项目根目录或调整路径）\n",
    "    DATA_DIR = \"diffusion_planner_devset\"  # 预处理数据文件夹\n",
    "    DEVICE = \"cpu\"\n",
    "    ENABLE_EMA = True  # 是否使用EMA权重\n",
    "\n",
    "    # ====================== 初始化推理器并执行批量推理 ======================\n",
    "    inferencer = DiffusionPlannerInfer(\n",
    "        config_path=CONFIG_PATH,\n",
    "        ckpt_path=CKPT_PATH,\n",
    "        device=DEVICE,\n",
    "        enable_ema=ENABLE_EMA\n",
    "    )\n",
    "    inferencer.infer_batch(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58bfa1b6-9989-4e28-a751-f59c454d9b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Diffusion_Planner:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.encoder.neighbor_encoder.type_emb.weight\", \"encoder.encoder.neighbor_encoder.type_emb.bias\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc2.bias\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.norm.weight\", \"encoder.encoder.neighbor_encoder.norm.bias\", \"encoder.encoder.neighbor_encoder.emb_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.emb_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.emb_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.emb_project.fc2.bias\", \"encoder.encoder.static_encoder.projection.fc1.weight\", \"encoder.encoder.static_encoder.projection.fc1.bias\", \"encoder.encoder.static_encoder.projection.fc2.weight\", \"encoder.encoder.static_encoder.projection.fc2.bias\", \"encoder.encoder.lane_encoder.speed_limit_emb.weight\", \"encoder.encoder.lane_encoder.speed_limit_emb.bias\", \"encoder.encoder.lane_encoder.unknown_speed_emb.weight\", \"encoder.encoder.lane_encoder.traffic_emb.weight\", \"encoder.encoder.lane_encoder.traffic_emb.bias\", \"encoder.encoder.lane_encoder.channel_pre_project.fc1.weight\", \"encoder.encoder.lane_encoder.channel_pre_project.fc1.bias\", \"encoder.encoder.lane_encoder.channel_pre_project.fc2.weight\", \"encoder.encoder.lane_encoder.channel_pre_project.fc2.bias\", \"encoder.encoder.lane_encoder.token_pre_project.fc1.weight\", \"encoder.encoder.lane_encoder.token_pre_project.fc1.bias\", \"encoder.encoder.lane_encoder.token_pre_project.fc2.weight\", \"encoder.encoder.lane_encoder.token_pre_project.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.0.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.0.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.0.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.0.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.1.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.1.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.1.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.1.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.2.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.2.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.2.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.2.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.norm.weight\", \"encoder.encoder.lane_encoder.norm.bias\", \"encoder.encoder.lane_encoder.emb_project.fc1.weight\", \"encoder.encoder.lane_encoder.emb_project.fc1.bias\", \"encoder.encoder.lane_encoder.emb_project.fc2.weight\", \"encoder.encoder.lane_encoder.emb_project.fc2.bias\", \"encoder.encoder.fusion.blocks.0.norm1.weight\", \"encoder.encoder.fusion.blocks.0.norm1.bias\", \"encoder.encoder.fusion.blocks.0.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.0.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.0.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.0.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.0.norm2.weight\", \"encoder.encoder.fusion.blocks.0.norm2.bias\", \"encoder.encoder.fusion.blocks.0.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.0.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.0.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.0.mlp.fc2.bias\", \"encoder.encoder.fusion.blocks.1.norm1.weight\", \"encoder.encoder.fusion.blocks.1.norm1.bias\", \"encoder.encoder.fusion.blocks.1.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.1.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.1.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.1.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.1.norm2.weight\", \"encoder.encoder.fusion.blocks.1.norm2.bias\", \"encoder.encoder.fusion.blocks.1.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.1.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.1.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.1.mlp.fc2.bias\", \"encoder.encoder.fusion.blocks.2.norm1.weight\", \"encoder.encoder.fusion.blocks.2.norm1.bias\", \"encoder.encoder.fusion.blocks.2.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.2.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.2.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.2.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.2.norm2.weight\", \"encoder.encoder.fusion.blocks.2.norm2.bias\", \"encoder.encoder.fusion.blocks.2.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.2.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.2.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.2.mlp.fc2.bias\", \"encoder.encoder.fusion.norm.weight\", \"encoder.encoder.fusion.norm.bias\", \"encoder.encoder.pos_emb.weight\", \"encoder.encoder.pos_emb.bias\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc2.bias\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.norm1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.norm1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.norm2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.norm2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc2.bias\", \"decoder.decoder.dit.route_encoder.norm.weight\", \"decoder.decoder.dit.route_encoder.norm.bias\", \"decoder.decoder.dit.route_encoder.emb_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.emb_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.emb_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.emb_project.fc2.bias\", \"decoder.decoder.dit.agent_embedding.weight\", \"decoder.decoder.dit.preproj.fc1.weight\", \"decoder.decoder.dit.preproj.fc1.bias\", \"decoder.decoder.dit.preproj.fc2.weight\", \"decoder.decoder.dit.preproj.fc2.bias\", \"decoder.decoder.dit.t_embedder.mlp.0.weight\", \"decoder.decoder.dit.t_embedder.mlp.0.bias\", \"decoder.decoder.dit.t_embedder.mlp.2.weight\", \"decoder.decoder.dit.t_embedder.mlp.2.bias\", \"decoder.decoder.dit.blocks.0.norm1.weight\", \"decoder.decoder.dit.blocks.0.norm1.bias\", \"decoder.decoder.dit.blocks.0.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.0.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.0.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.0.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.0.norm2.weight\", \"decoder.decoder.dit.blocks.0.norm2.bias\", \"decoder.decoder.dit.blocks.0.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.0.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.0.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.0.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.0.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.0.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.0.norm3.weight\", \"decoder.decoder.dit.blocks.0.norm3.bias\", \"decoder.decoder.dit.blocks.0.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.0.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.0.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.0.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.0.norm4.weight\", \"decoder.decoder.dit.blocks.0.norm4.bias\", \"decoder.decoder.dit.blocks.0.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.0.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.0.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.0.mlp2.fc2.bias\", \"decoder.decoder.dit.blocks.1.norm1.weight\", \"decoder.decoder.dit.blocks.1.norm1.bias\", \"decoder.decoder.dit.blocks.1.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.1.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.1.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.1.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.1.norm2.weight\", \"decoder.decoder.dit.blocks.1.norm2.bias\", \"decoder.decoder.dit.blocks.1.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.1.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.1.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.1.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.1.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.1.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.1.norm3.weight\", \"decoder.decoder.dit.blocks.1.norm3.bias\", \"decoder.decoder.dit.blocks.1.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.1.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.1.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.1.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.1.norm4.weight\", \"decoder.decoder.dit.blocks.1.norm4.bias\", \"decoder.decoder.dit.blocks.1.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.1.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.1.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.1.mlp2.fc2.bias\", \"decoder.decoder.dit.blocks.2.norm1.weight\", \"decoder.decoder.dit.blocks.2.norm1.bias\", \"decoder.decoder.dit.blocks.2.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.2.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.2.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.2.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.2.norm2.weight\", \"decoder.decoder.dit.blocks.2.norm2.bias\", \"decoder.decoder.dit.blocks.2.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.2.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.2.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.2.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.2.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.2.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.2.norm3.weight\", \"decoder.decoder.dit.blocks.2.norm3.bias\", \"decoder.decoder.dit.blocks.2.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.2.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.2.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.2.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.2.norm4.weight\", \"decoder.decoder.dit.blocks.2.norm4.bias\", \"decoder.decoder.dit.blocks.2.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.2.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.2.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.2.mlp2.fc2.bias\", \"decoder.decoder.dit.final_layer.norm_final.weight\", \"decoder.decoder.dit.final_layer.norm_final.bias\", \"decoder.decoder.dit.final_layer.proj.0.weight\", \"decoder.decoder.dit.final_layer.proj.0.bias\", \"decoder.decoder.dit.final_layer.proj.1.weight\", \"decoder.decoder.dit.final_layer.proj.1.bias\", \"decoder.decoder.dit.final_layer.proj.3.weight\", \"decoder.decoder.dit.final_layer.proj.3.bias\", \"decoder.decoder.dit.final_layer.proj.4.weight\", \"decoder.decoder.dit.final_layer.proj.4.bias\", \"decoder.decoder.dit.final_layer.adaLN_modulation.1.weight\", \"decoder.decoder.dit.final_layer.adaLN_modulation.1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# -------------------------- 执行推理 --------------------------\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m加载模型...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCKPT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始推理...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    120\u001b[0m batch_infer(model, DATA_DIR, DEVICE)\n",
      "Cell \u001b[1;32mIn[2], line 67\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(ckpt_path, config, device)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# 处理多卡训练的\"module.\"前缀\u001b[39;00m\n\u001b[0;32m     66\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m {k[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m\"\u001b[39m):]: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m---> 67\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Anaconda\\anaconda\\envs\\nuplan_py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Diffusion_Planner:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.encoder.neighbor_encoder.type_emb.weight\", \"encoder.encoder.neighbor_encoder.type_emb.bias\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc2.bias\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.norm.weight\", \"encoder.encoder.neighbor_encoder.norm.bias\", \"encoder.encoder.neighbor_encoder.emb_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.emb_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.emb_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.emb_project.fc2.bias\", \"encoder.encoder.static_encoder.projection.fc1.weight\", \"encoder.encoder.static_encoder.projection.fc1.bias\", \"encoder.encoder.static_encoder.projection.fc2.weight\", \"encoder.encoder.static_encoder.projection.fc2.bias\", \"encoder.encoder.lane_encoder.speed_limit_emb.weight\", \"encoder.encoder.lane_encoder.speed_limit_emb.bias\", \"encoder.encoder.lane_encoder.unknown_speed_emb.weight\", \"encoder.encoder.lane_encoder.traffic_emb.weight\", \"encoder.encoder.lane_encoder.traffic_emb.bias\", \"encoder.encoder.lane_encoder.channel_pre_project.fc1.weight\", \"encoder.encoder.lane_encoder.channel_pre_project.fc1.bias\", \"encoder.encoder.lane_encoder.channel_pre_project.fc2.weight\", \"encoder.encoder.lane_encoder.channel_pre_project.fc2.bias\", \"encoder.encoder.lane_encoder.token_pre_project.fc1.weight\", \"encoder.encoder.lane_encoder.token_pre_project.fc1.bias\", \"encoder.encoder.lane_encoder.token_pre_project.fc2.weight\", \"encoder.encoder.lane_encoder.token_pre_project.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.0.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.0.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.0.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.0.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.1.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.1.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.1.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.1.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.2.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.2.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.2.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.2.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.norm.weight\", \"encoder.encoder.lane_encoder.norm.bias\", \"encoder.encoder.lane_encoder.emb_project.fc1.weight\", \"encoder.encoder.lane_encoder.emb_project.fc1.bias\", \"encoder.encoder.lane_encoder.emb_project.fc2.weight\", \"encoder.encoder.lane_encoder.emb_project.fc2.bias\", \"encoder.encoder.fusion.blocks.0.norm1.weight\", \"encoder.encoder.fusion.blocks.0.norm1.bias\", \"encoder.encoder.fusion.blocks.0.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.0.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.0.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.0.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.0.norm2.weight\", \"encoder.encoder.fusion.blocks.0.norm2.bias\", \"encoder.encoder.fusion.blocks.0.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.0.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.0.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.0.mlp.fc2.bias\", \"encoder.encoder.fusion.blocks.1.norm1.weight\", \"encoder.encoder.fusion.blocks.1.norm1.bias\", \"encoder.encoder.fusion.blocks.1.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.1.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.1.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.1.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.1.norm2.weight\", \"encoder.encoder.fusion.blocks.1.norm2.bias\", \"encoder.encoder.fusion.blocks.1.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.1.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.1.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.1.mlp.fc2.bias\", \"encoder.encoder.fusion.blocks.2.norm1.weight\", \"encoder.encoder.fusion.blocks.2.norm1.bias\", \"encoder.encoder.fusion.blocks.2.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.2.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.2.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.2.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.2.norm2.weight\", \"encoder.encoder.fusion.blocks.2.norm2.bias\", \"encoder.encoder.fusion.blocks.2.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.2.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.2.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.2.mlp.fc2.bias\", \"encoder.encoder.fusion.norm.weight\", \"encoder.encoder.fusion.norm.bias\", \"encoder.encoder.pos_emb.weight\", \"encoder.encoder.pos_emb.bias\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc2.bias\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.norm1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.norm1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.norm2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.norm2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc2.bias\", \"decoder.decoder.dit.route_encoder.norm.weight\", \"decoder.decoder.dit.route_encoder.norm.bias\", \"decoder.decoder.dit.route_encoder.emb_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.emb_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.emb_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.emb_project.fc2.bias\", \"decoder.decoder.dit.agent_embedding.weight\", \"decoder.decoder.dit.preproj.fc1.weight\", \"decoder.decoder.dit.preproj.fc1.bias\", \"decoder.decoder.dit.preproj.fc2.weight\", \"decoder.decoder.dit.preproj.fc2.bias\", \"decoder.decoder.dit.t_embedder.mlp.0.weight\", \"decoder.decoder.dit.t_embedder.mlp.0.bias\", \"decoder.decoder.dit.t_embedder.mlp.2.weight\", \"decoder.decoder.dit.t_embedder.mlp.2.bias\", \"decoder.decoder.dit.blocks.0.norm1.weight\", \"decoder.decoder.dit.blocks.0.norm1.bias\", \"decoder.decoder.dit.blocks.0.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.0.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.0.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.0.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.0.norm2.weight\", \"decoder.decoder.dit.blocks.0.norm2.bias\", \"decoder.decoder.dit.blocks.0.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.0.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.0.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.0.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.0.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.0.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.0.norm3.weight\", \"decoder.decoder.dit.blocks.0.norm3.bias\", \"decoder.decoder.dit.blocks.0.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.0.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.0.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.0.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.0.norm4.weight\", \"decoder.decoder.dit.blocks.0.norm4.bias\", \"decoder.decoder.dit.blocks.0.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.0.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.0.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.0.mlp2.fc2.bias\", \"decoder.decoder.dit.blocks.1.norm1.weight\", \"decoder.decoder.dit.blocks.1.norm1.bias\", \"decoder.decoder.dit.blocks.1.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.1.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.1.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.1.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.1.norm2.weight\", \"decoder.decoder.dit.blocks.1.norm2.bias\", \"decoder.decoder.dit.blocks.1.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.1.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.1.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.1.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.1.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.1.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.1.norm3.weight\", \"decoder.decoder.dit.blocks.1.norm3.bias\", \"decoder.decoder.dit.blocks.1.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.1.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.1.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.1.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.1.norm4.weight\", \"decoder.decoder.dit.blocks.1.norm4.bias\", \"decoder.decoder.dit.blocks.1.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.1.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.1.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.1.mlp2.fc2.bias\", \"decoder.decoder.dit.blocks.2.norm1.weight\", \"decoder.decoder.dit.blocks.2.norm1.bias\", \"decoder.decoder.dit.blocks.2.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.2.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.2.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.2.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.2.norm2.weight\", \"decoder.decoder.dit.blocks.2.norm2.bias\", \"decoder.decoder.dit.blocks.2.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.2.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.2.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.2.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.2.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.2.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.2.norm3.weight\", \"decoder.decoder.dit.blocks.2.norm3.bias\", \"decoder.decoder.dit.blocks.2.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.2.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.2.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.2.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.2.norm4.weight\", \"decoder.decoder.dit.blocks.2.norm4.bias\", \"decoder.decoder.dit.blocks.2.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.2.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.2.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.2.mlp2.fc2.bias\", \"decoder.decoder.dit.final_layer.norm_final.weight\", \"decoder.decoder.dit.final_layer.norm_final.bias\", \"decoder.decoder.dit.final_layer.proj.0.weight\", \"decoder.decoder.dit.final_layer.proj.0.bias\", \"decoder.decoder.dit.final_layer.proj.1.weight\", \"decoder.decoder.dit.final_layer.proj.1.bias\", \"decoder.decoder.dit.final_layer.proj.3.weight\", \"decoder.decoder.dit.final_layer.proj.3.bias\", \"decoder.decoder.dit.final_layer.proj.4.weight\", \"decoder.decoder.dit.final_layer.proj.4.bias\", \"decoder.decoder.dit.final_layer.adaLN_modulation.1.weight\", \"decoder.decoder.dit.final_layer.adaLN_modulation.1.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "# ====================== 1. 模型类（必须与训练时结构完全一致） ======================\n",
    "class Diffusion_Planner(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 此处填入你训练时的模型结构（从 diffusion_planner/model/diffusion_planner.py 复制）\n",
    "        # 示例：假设模型结构如下（需替换为你的真实结构）\n",
    "        self.config = config\n",
    "        self.fc1 = torch.nn.Linear(config.input_dim, config.hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(config.hidden_dim, config.output_dim * config.num_steps)\n",
    "        self.num_steps = config.num_steps\n",
    "        self.output_dim = config.output_dim\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]):\n",
    "        # 此处填入你训练时的 forward 逻辑（从原模型复制）\n",
    "        x = self.fc1(inputs[\"history\"].mean(dim=1))  # 示例：简单处理\n",
    "        pred = self.fc2(x).view(-1, 1, self.num_steps, self.output_dim)\n",
    "        return None, {\"prediction\": pred}\n",
    "\n",
    "\n",
    "# ====================== 2. 极简数据处理器（仅做格式转换） ======================\n",
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config  # 仅保留配置，无实际处理逻辑\n",
    "\n",
    "    def observation_adapter(self, npz_data: dict, device: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        仅将npz中的numpy数组转换为模型输入的Tensor，不做任何额外处理\n",
    "        键名必须与模型forward所需的输入键完全匹配\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # 替换为你的模型实际需要的输入键（与npz中的键对应）\n",
    "            \"history\": torch.tensor(npz_data[\"history\"], dtype=torch.float32).to(device),\n",
    "            \"traffic_light\": torch.tensor(npz_data[\"traffic_light\"], dtype=torch.float32).to(device),\n",
    "            # 如有其他输入键（如\"map\"），在此补充\n",
    "        }\n",
    "\n",
    "\n",
    "# ====================== 3. 配置类（仅存储模型参数） ======================\n",
    "class Config:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_steps):\n",
    "        self.input_dim = input_dim      # 输入特征维度（必须与训练一致）\n",
    "        self.hidden_dim = hidden_dim    # 隐藏层维度（必须与训练一致）\n",
    "        self.output_dim = output_dim    # 输出维度（如4：x,y,vx,vy）\n",
    "        self.num_steps = num_steps      # 预测时间步数（必须与训练一致）\n",
    "\n",
    "\n",
    "# ====================== 4. 推理主逻辑 ======================\n",
    "def load_model(ckpt_path: str, config: Config, device: str = \"cpu\") -> Diffusion_Planner:\n",
    "    \"\"\"加载模型权重（处理EMA和多卡前缀）\"\"\"\n",
    "    model = Diffusion_Planner(config)\n",
    "    state_dict = torch.load(ckpt_path, map_location=device)\n",
    "    \n",
    "    # 处理EMA或模型权重\n",
    "    if \"ema_state_dict\" in state_dict:\n",
    "        state_dict = state_dict[\"ema_state_dict\"]\n",
    "    elif \"model\" in state_dict:\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    \n",
    "    # 处理多卡训练的\"module.\"前缀\n",
    "    model_state_dict = {k[len(\"module.\"):]: v for k, v in state_dict.items() if k.startswith(\"module.\")}\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def infer_single(model: Diffusion_Planner, npz_path: str, device: str) -> np.ndarray:\n",
    "    \"\"\"对单个npz文件执行推理（数据直接可用）\"\"\"\n",
    "    # 1. 加载npz数据（已预处理，可直接用）\n",
    "    npz_data = np.load(npz_path)\n",
    "    \n",
    "    # 2. 转换为模型输入Tensor（仅格式转换）\n",
    "    processor = DataProcessor(model.config)\n",
    "    inputs = processor.observation_adapter(npz_data, device)\n",
    "    \n",
    "    # 3. 模型推理（无梯度计算）\n",
    "    with torch.no_grad():\n",
    "        _, outputs = model(inputs)\n",
    "    \n",
    "    # 4. 提取预测结果（根据模型输出格式调整）\n",
    "    return outputs[\"prediction\"][0, 0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def batch_infer(model: Diffusion_Planner, data_dir: str, device: str):\n",
    "    \"\"\"批量推理文件夹中的所有npz文件\"\"\"\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".npz\"):\n",
    "            npz_path = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                pred = infer_single(model, npz_path, device)\n",
    "                print(f\"✅ {filename} | 预测形状: {pred.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {filename} | 错误: {str(e)}\")\n",
    "\n",
    "\n",
    "# ====================== 5. 执行入口 ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # -------------------------- 配置参数（必须与训练时一致！） --------------------------\n",
    "    DEVICE = \"cpu\"\n",
    "    CKPT_PATH = \"model.pth\"          # 模型权重路径\n",
    "    DATA_DIR = \"diffusion_planner_devset\"  # 预处理数据文件夹\n",
    "    # 模型参数（从训练配置中获取，必须完全一致）\n",
    "    MODEL_CONFIG = Config(\n",
    "        input_dim=128,    # 输入特征维度\n",
    "        hidden_dim=256,   # 隐藏层维度\n",
    "        output_dim=4,     # 输出维度（x,y,vx,vy）\n",
    "        num_steps=20      # 预测时间步数\n",
    "    )\n",
    "\n",
    "    # -------------------------- 执行推理 --------------------------\n",
    "    print(\"加载模型...\")\n",
    "    model = load_model(CKPT_PATH, MODEL_CONFIG, DEVICE)\n",
    "    \n",
    "    print(\"开始推理...\")\n",
    "    batch_infer(model, DATA_DIR, DEVICE)\n",
    "    print(\"推理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c551337-0b19-410a-b265-0d3bc37ff58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 初始化训练配置（与 args.json 完全一致）...\n",
      "2. 加载训练好的模型...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Diffusion_Planner:\n\tMissing key(s) in state_dict: \"agent_emb.weight\", \"agent_emb.bias\", \"lane_emb.weight\", \"lane_emb.bias\", \"static_emb.weight\", \"static_emb.bias\", \"route_emb.weight\", \"route_emb.bias\", \"encoder.layers.0.self_attn.in_proj_weight\", \"encoder.layers.0.self_attn.in_proj_bias\", \"encoder.layers.0.self_attn.out_proj.weight\", \"encoder.layers.0.self_attn.out_proj.bias\", \"encoder.layers.0.linear1.weight\", \"encoder.layers.0.linear1.bias\", \"encoder.layers.0.linear2.weight\", \"encoder.layers.0.linear2.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.1.self_attn.in_proj_weight\", \"encoder.layers.1.self_attn.in_proj_bias\", \"encoder.layers.1.self_attn.out_proj.weight\", \"encoder.layers.1.self_attn.out_proj.bias\", \"encoder.layers.1.linear1.weight\", \"encoder.layers.1.linear1.bias\", \"encoder.layers.1.linear2.weight\", \"encoder.layers.1.linear2.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.2.self_attn.in_proj_weight\", \"encoder.layers.2.self_attn.in_proj_bias\", \"encoder.layers.2.self_attn.out_proj.weight\", \"encoder.layers.2.self_attn.out_proj.bias\", \"encoder.layers.2.linear1.weight\", \"encoder.layers.2.linear1.bias\", \"encoder.layers.2.linear2.weight\", \"encoder.layers.2.linear2.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"decoder.layers.0.self_attn.in_proj_weight\", \"decoder.layers.0.self_attn.in_proj_bias\", \"decoder.layers.0.self_attn.out_proj.weight\", \"decoder.layers.0.self_attn.out_proj.bias\", \"decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.layers.0.linear1.weight\", \"decoder.layers.0.linear1.bias\", \"decoder.layers.0.linear2.weight\", \"decoder.layers.0.linear2.bias\", \"decoder.layers.0.norm1.weight\", \"decoder.layers.0.norm1.bias\", \"decoder.layers.0.norm2.weight\", \"decoder.layers.0.norm2.bias\", \"decoder.layers.0.norm3.weight\", \"decoder.layers.0.norm3.bias\", \"decoder.layers.1.self_attn.in_proj_weight\", \"decoder.layers.1.self_attn.in_proj_bias\", \"decoder.layers.1.self_attn.out_proj.weight\", \"decoder.layers.1.self_attn.out_proj.bias\", \"decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.layers.1.linear1.weight\", \"decoder.layers.1.linear1.bias\", \"decoder.layers.1.linear2.weight\", \"decoder.layers.1.linear2.bias\", \"decoder.layers.1.norm1.weight\", \"decoder.layers.1.norm1.bias\", \"decoder.layers.1.norm2.weight\", \"decoder.layers.1.norm2.bias\", \"decoder.layers.1.norm3.weight\", \"decoder.layers.1.norm3.bias\", \"decoder.layers.2.self_attn.in_proj_weight\", \"decoder.layers.2.self_attn.in_proj_bias\", \"decoder.layers.2.self_attn.out_proj.weight\", \"decoder.layers.2.self_attn.out_proj.bias\", \"decoder.layers.2.multihead_attn.in_proj_weight\", \"decoder.layers.2.multihead_attn.in_proj_bias\", \"decoder.layers.2.multihead_attn.out_proj.weight\", \"decoder.layers.2.multihead_attn.out_proj.bias\", \"decoder.layers.2.linear1.weight\", \"decoder.layers.2.linear1.bias\", \"decoder.layers.2.linear2.weight\", \"decoder.layers.2.linear2.bias\", \"decoder.layers.2.norm1.weight\", \"decoder.layers.2.norm1.bias\", \"decoder.layers.2.norm2.weight\", \"decoder.layers.2.norm2.bias\", \"decoder.layers.2.norm3.weight\", \"decoder.layers.2.norm3.bias\", \"output_layer.weight\", \"output_layer.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.encoder.neighbor_encoder.type_emb.weight\", \"encoder.encoder.neighbor_encoder.type_emb.bias\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc2.bias\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.norm.weight\", \"encoder.encoder.neighbor_encoder.norm.bias\", \"encoder.encoder.neighbor_encoder.emb_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.emb_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.emb_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.emb_project.fc2.bias\", \"encoder.encoder.static_encoder.projection.fc1.weight\", \"encoder.encoder.static_encoder.projection.fc1.bias\", \"encoder.encoder.static_encoder.projection.fc2.weight\", \"encoder.encoder.static_encoder.projection.fc2.bias\", \"encoder.encoder.lane_encoder.speed_limit_emb.weight\", \"encoder.encoder.lane_encoder.speed_limit_emb.bias\", \"encoder.encoder.lane_encoder.unknown_speed_emb.weight\", \"encoder.encoder.lane_encoder.traffic_emb.weight\", \"encoder.encoder.lane_encoder.traffic_emb.bias\", \"encoder.encoder.lane_encoder.channel_pre_project.fc1.weight\", \"encoder.encoder.lane_encoder.channel_pre_project.fc1.bias\", \"encoder.encoder.lane_encoder.channel_pre_project.fc2.weight\", \"encoder.encoder.lane_encoder.channel_pre_project.fc2.bias\", \"encoder.encoder.lane_encoder.token_pre_project.fc1.weight\", \"encoder.encoder.lane_encoder.token_pre_project.fc1.bias\", \"encoder.encoder.lane_encoder.token_pre_project.fc2.weight\", \"encoder.encoder.lane_encoder.token_pre_project.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.0.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.0.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.0.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.0.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.1.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.1.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.1.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.1.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.2.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.2.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.2.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.2.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.norm.weight\", \"encoder.encoder.lane_encoder.norm.bias\", \"encoder.encoder.lane_encoder.emb_project.fc1.weight\", \"encoder.encoder.lane_encoder.emb_project.fc1.bias\", \"encoder.encoder.lane_encoder.emb_project.fc2.weight\", \"encoder.encoder.lane_encoder.emb_project.fc2.bias\", \"encoder.encoder.fusion.blocks.0.norm1.weight\", \"encoder.encoder.fusion.blocks.0.norm1.bias\", \"encoder.encoder.fusion.blocks.0.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.0.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.0.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.0.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.0.norm2.weight\", \"encoder.encoder.fusion.blocks.0.norm2.bias\", \"encoder.encoder.fusion.blocks.0.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.0.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.0.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.0.mlp.fc2.bias\", \"encoder.encoder.fusion.blocks.1.norm1.weight\", \"encoder.encoder.fusion.blocks.1.norm1.bias\", \"encoder.encoder.fusion.blocks.1.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.1.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.1.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.1.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.1.norm2.weight\", \"encoder.encoder.fusion.blocks.1.norm2.bias\", \"encoder.encoder.fusion.blocks.1.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.1.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.1.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.1.mlp.fc2.bias\", \"encoder.encoder.fusion.blocks.2.norm1.weight\", \"encoder.encoder.fusion.blocks.2.norm1.bias\", \"encoder.encoder.fusion.blocks.2.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.2.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.2.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.2.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.2.norm2.weight\", \"encoder.encoder.fusion.blocks.2.norm2.bias\", \"encoder.encoder.fusion.blocks.2.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.2.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.2.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.2.mlp.fc2.bias\", \"encoder.encoder.fusion.norm.weight\", \"encoder.encoder.fusion.norm.bias\", \"encoder.encoder.pos_emb.weight\", \"encoder.encoder.pos_emb.bias\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc2.bias\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.norm1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.norm1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.norm2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.norm2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc2.bias\", \"decoder.decoder.dit.route_encoder.norm.weight\", \"decoder.decoder.dit.route_encoder.norm.bias\", \"decoder.decoder.dit.route_encoder.emb_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.emb_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.emb_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.emb_project.fc2.bias\", \"decoder.decoder.dit.agent_embedding.weight\", \"decoder.decoder.dit.preproj.fc1.weight\", \"decoder.decoder.dit.preproj.fc1.bias\", \"decoder.decoder.dit.preproj.fc2.weight\", \"decoder.decoder.dit.preproj.fc2.bias\", \"decoder.decoder.dit.t_embedder.mlp.0.weight\", \"decoder.decoder.dit.t_embedder.mlp.0.bias\", \"decoder.decoder.dit.t_embedder.mlp.2.weight\", \"decoder.decoder.dit.t_embedder.mlp.2.bias\", \"decoder.decoder.dit.blocks.0.norm1.weight\", \"decoder.decoder.dit.blocks.0.norm1.bias\", \"decoder.decoder.dit.blocks.0.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.0.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.0.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.0.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.0.norm2.weight\", \"decoder.decoder.dit.blocks.0.norm2.bias\", \"decoder.decoder.dit.blocks.0.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.0.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.0.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.0.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.0.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.0.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.0.norm3.weight\", \"decoder.decoder.dit.blocks.0.norm3.bias\", \"decoder.decoder.dit.blocks.0.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.0.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.0.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.0.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.0.norm4.weight\", \"decoder.decoder.dit.blocks.0.norm4.bias\", \"decoder.decoder.dit.blocks.0.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.0.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.0.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.0.mlp2.fc2.bias\", \"decoder.decoder.dit.blocks.1.norm1.weight\", \"decoder.decoder.dit.blocks.1.norm1.bias\", \"decoder.decoder.dit.blocks.1.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.1.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.1.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.1.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.1.norm2.weight\", \"decoder.decoder.dit.blocks.1.norm2.bias\", \"decoder.decoder.dit.blocks.1.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.1.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.1.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.1.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.1.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.1.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.1.norm3.weight\", \"decoder.decoder.dit.blocks.1.norm3.bias\", \"decoder.decoder.dit.blocks.1.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.1.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.1.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.1.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.1.norm4.weight\", \"decoder.decoder.dit.blocks.1.norm4.bias\", \"decoder.decoder.dit.blocks.1.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.1.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.1.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.1.mlp2.fc2.bias\", \"decoder.decoder.dit.blocks.2.norm1.weight\", \"decoder.decoder.dit.blocks.2.norm1.bias\", \"decoder.decoder.dit.blocks.2.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.2.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.2.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.2.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.2.norm2.weight\", \"decoder.decoder.dit.blocks.2.norm2.bias\", \"decoder.decoder.dit.blocks.2.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.2.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.2.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.2.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.2.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.2.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.2.norm3.weight\", \"decoder.decoder.dit.blocks.2.norm3.bias\", \"decoder.decoder.dit.blocks.2.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.2.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.2.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.2.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.2.norm4.weight\", \"decoder.decoder.dit.blocks.2.norm4.bias\", \"decoder.decoder.dit.blocks.2.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.2.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.2.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.2.mlp2.fc2.bias\", \"decoder.decoder.dit.final_layer.norm_final.weight\", \"decoder.decoder.dit.final_layer.norm_final.bias\", \"decoder.decoder.dit.final_layer.proj.0.weight\", \"decoder.decoder.dit.final_layer.proj.0.bias\", \"decoder.decoder.dit.final_layer.proj.1.weight\", \"decoder.decoder.dit.final_layer.proj.1.bias\", \"decoder.decoder.dit.final_layer.proj.3.weight\", \"decoder.decoder.dit.final_layer.proj.3.bias\", \"decoder.decoder.dit.final_layer.proj.4.weight\", \"decoder.decoder.dit.final_layer.proj.4.bias\", \"decoder.decoder.dit.final_layer.adaLN_modulation.1.weight\", \"decoder.decoder.dit.final_layer.adaLN_modulation.1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 209\u001b[0m\n\u001b[0;32m    206\u001b[0m config \u001b[38;5;241m=\u001b[39m Config()\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. 加载训练好的模型...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 209\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_training_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCKPT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. 开始批量推理...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m batch_infer(model, DATA_DIR, DEVICE)\n",
      "Cell \u001b[1;32mIn[3], line 159\u001b[0m, in \u001b[0;36mload_training_model\u001b[1;34m(ckpt_path, config, device)\u001b[0m\n\u001b[0;32m    156\u001b[0m         model_state_dict[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# 加载权重（确保模型结构与训练完全一致，否则会报错）\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# 切换到推理模式（关闭Dropout等训练特有的层）\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Anaconda\\anaconda\\envs\\nuplan_py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Diffusion_Planner:\n\tMissing key(s) in state_dict: \"agent_emb.weight\", \"agent_emb.bias\", \"lane_emb.weight\", \"lane_emb.bias\", \"static_emb.weight\", \"static_emb.bias\", \"route_emb.weight\", \"route_emb.bias\", \"encoder.layers.0.self_attn.in_proj_weight\", \"encoder.layers.0.self_attn.in_proj_bias\", \"encoder.layers.0.self_attn.out_proj.weight\", \"encoder.layers.0.self_attn.out_proj.bias\", \"encoder.layers.0.linear1.weight\", \"encoder.layers.0.linear1.bias\", \"encoder.layers.0.linear2.weight\", \"encoder.layers.0.linear2.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.1.self_attn.in_proj_weight\", \"encoder.layers.1.self_attn.in_proj_bias\", \"encoder.layers.1.self_attn.out_proj.weight\", \"encoder.layers.1.self_attn.out_proj.bias\", \"encoder.layers.1.linear1.weight\", \"encoder.layers.1.linear1.bias\", \"encoder.layers.1.linear2.weight\", \"encoder.layers.1.linear2.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.2.self_attn.in_proj_weight\", \"encoder.layers.2.self_attn.in_proj_bias\", \"encoder.layers.2.self_attn.out_proj.weight\", \"encoder.layers.2.self_attn.out_proj.bias\", \"encoder.layers.2.linear1.weight\", \"encoder.layers.2.linear1.bias\", \"encoder.layers.2.linear2.weight\", \"encoder.layers.2.linear2.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"decoder.layers.0.self_attn.in_proj_weight\", \"decoder.layers.0.self_attn.in_proj_bias\", \"decoder.layers.0.self_attn.out_proj.weight\", \"decoder.layers.0.self_attn.out_proj.bias\", \"decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.layers.0.linear1.weight\", \"decoder.layers.0.linear1.bias\", \"decoder.layers.0.linear2.weight\", \"decoder.layers.0.linear2.bias\", \"decoder.layers.0.norm1.weight\", \"decoder.layers.0.norm1.bias\", \"decoder.layers.0.norm2.weight\", \"decoder.layers.0.norm2.bias\", \"decoder.layers.0.norm3.weight\", \"decoder.layers.0.norm3.bias\", \"decoder.layers.1.self_attn.in_proj_weight\", \"decoder.layers.1.self_attn.in_proj_bias\", \"decoder.layers.1.self_attn.out_proj.weight\", \"decoder.layers.1.self_attn.out_proj.bias\", \"decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.layers.1.linear1.weight\", \"decoder.layers.1.linear1.bias\", \"decoder.layers.1.linear2.weight\", \"decoder.layers.1.linear2.bias\", \"decoder.layers.1.norm1.weight\", \"decoder.layers.1.norm1.bias\", \"decoder.layers.1.norm2.weight\", \"decoder.layers.1.norm2.bias\", \"decoder.layers.1.norm3.weight\", \"decoder.layers.1.norm3.bias\", \"decoder.layers.2.self_attn.in_proj_weight\", \"decoder.layers.2.self_attn.in_proj_bias\", \"decoder.layers.2.self_attn.out_proj.weight\", \"decoder.layers.2.self_attn.out_proj.bias\", \"decoder.layers.2.multihead_attn.in_proj_weight\", \"decoder.layers.2.multihead_attn.in_proj_bias\", \"decoder.layers.2.multihead_attn.out_proj.weight\", \"decoder.layers.2.multihead_attn.out_proj.bias\", \"decoder.layers.2.linear1.weight\", \"decoder.layers.2.linear1.bias\", \"decoder.layers.2.linear2.weight\", \"decoder.layers.2.linear2.bias\", \"decoder.layers.2.norm1.weight\", \"decoder.layers.2.norm1.bias\", \"decoder.layers.2.norm2.weight\", \"decoder.layers.2.norm2.bias\", \"decoder.layers.2.norm3.weight\", \"decoder.layers.2.norm3.bias\", \"output_layer.weight\", \"output_layer.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.encoder.neighbor_encoder.type_emb.weight\", \"encoder.encoder.neighbor_encoder.type_emb.bias\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.channel_pre_project.fc2.bias\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.token_pre_project.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.0.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.1.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.norm1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.norm1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.channels_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.norm2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.norm2.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc1.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc1.bias\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc2.weight\", \"encoder.encoder.neighbor_encoder.blocks.2.tokens_mlp.fc2.bias\", \"encoder.encoder.neighbor_encoder.norm.weight\", \"encoder.encoder.neighbor_encoder.norm.bias\", \"encoder.encoder.neighbor_encoder.emb_project.fc1.weight\", \"encoder.encoder.neighbor_encoder.emb_project.fc1.bias\", \"encoder.encoder.neighbor_encoder.emb_project.fc2.weight\", \"encoder.encoder.neighbor_encoder.emb_project.fc2.bias\", \"encoder.encoder.static_encoder.projection.fc1.weight\", \"encoder.encoder.static_encoder.projection.fc1.bias\", \"encoder.encoder.static_encoder.projection.fc2.weight\", \"encoder.encoder.static_encoder.projection.fc2.bias\", \"encoder.encoder.lane_encoder.speed_limit_emb.weight\", \"encoder.encoder.lane_encoder.speed_limit_emb.bias\", \"encoder.encoder.lane_encoder.unknown_speed_emb.weight\", \"encoder.encoder.lane_encoder.traffic_emb.weight\", \"encoder.encoder.lane_encoder.traffic_emb.bias\", \"encoder.encoder.lane_encoder.channel_pre_project.fc1.weight\", \"encoder.encoder.lane_encoder.channel_pre_project.fc1.bias\", \"encoder.encoder.lane_encoder.channel_pre_project.fc2.weight\", \"encoder.encoder.lane_encoder.channel_pre_project.fc2.bias\", \"encoder.encoder.lane_encoder.token_pre_project.fc1.weight\", \"encoder.encoder.lane_encoder.token_pre_project.fc1.bias\", \"encoder.encoder.lane_encoder.token_pre_project.fc2.weight\", \"encoder.encoder.lane_encoder.token_pre_project.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.0.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.0.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.0.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.0.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.0.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.0.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.1.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.1.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.1.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.1.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.1.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.1.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.2.norm1.weight\", \"encoder.encoder.lane_encoder.blocks.2.norm1.bias\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.2.channels_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.blocks.2.norm2.weight\", \"encoder.encoder.lane_encoder.blocks.2.norm2.bias\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc1.weight\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc1.bias\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc2.weight\", \"encoder.encoder.lane_encoder.blocks.2.tokens_mlp.fc2.bias\", \"encoder.encoder.lane_encoder.norm.weight\", \"encoder.encoder.lane_encoder.norm.bias\", \"encoder.encoder.lane_encoder.emb_project.fc1.weight\", \"encoder.encoder.lane_encoder.emb_project.fc1.bias\", \"encoder.encoder.lane_encoder.emb_project.fc2.weight\", \"encoder.encoder.lane_encoder.emb_project.fc2.bias\", \"encoder.encoder.fusion.blocks.0.norm1.weight\", \"encoder.encoder.fusion.blocks.0.norm1.bias\", \"encoder.encoder.fusion.blocks.0.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.0.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.0.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.0.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.0.norm2.weight\", \"encoder.encoder.fusion.blocks.0.norm2.bias\", \"encoder.encoder.fusion.blocks.0.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.0.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.0.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.0.mlp.fc2.bias\", \"encoder.encoder.fusion.blocks.1.norm1.weight\", \"encoder.encoder.fusion.blocks.1.norm1.bias\", \"encoder.encoder.fusion.blocks.1.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.1.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.1.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.1.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.1.norm2.weight\", \"encoder.encoder.fusion.blocks.1.norm2.bias\", \"encoder.encoder.fusion.blocks.1.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.1.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.1.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.1.mlp.fc2.bias\", \"encoder.encoder.fusion.blocks.2.norm1.weight\", \"encoder.encoder.fusion.blocks.2.norm1.bias\", \"encoder.encoder.fusion.blocks.2.attn.in_proj_weight\", \"encoder.encoder.fusion.blocks.2.attn.in_proj_bias\", \"encoder.encoder.fusion.blocks.2.attn.out_proj.weight\", \"encoder.encoder.fusion.blocks.2.attn.out_proj.bias\", \"encoder.encoder.fusion.blocks.2.norm2.weight\", \"encoder.encoder.fusion.blocks.2.norm2.bias\", \"encoder.encoder.fusion.blocks.2.mlp.fc1.weight\", \"encoder.encoder.fusion.blocks.2.mlp.fc1.bias\", \"encoder.encoder.fusion.blocks.2.mlp.fc2.weight\", \"encoder.encoder.fusion.blocks.2.mlp.fc2.bias\", \"encoder.encoder.fusion.norm.weight\", \"encoder.encoder.fusion.norm.bias\", \"encoder.encoder.pos_emb.weight\", \"encoder.encoder.pos_emb.bias\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.channel_pre_project.fc2.bias\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.token_pre_project.fc2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.norm1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.norm1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.channels_mlp.fc2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.norm2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.norm2.bias\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc1.weight\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc1.bias\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc2.weight\", \"decoder.decoder.dit.route_encoder.Mixer.tokens_mlp.fc2.bias\", \"decoder.decoder.dit.route_encoder.norm.weight\", \"decoder.decoder.dit.route_encoder.norm.bias\", \"decoder.decoder.dit.route_encoder.emb_project.fc1.weight\", \"decoder.decoder.dit.route_encoder.emb_project.fc1.bias\", \"decoder.decoder.dit.route_encoder.emb_project.fc2.weight\", \"decoder.decoder.dit.route_encoder.emb_project.fc2.bias\", \"decoder.decoder.dit.agent_embedding.weight\", \"decoder.decoder.dit.preproj.fc1.weight\", \"decoder.decoder.dit.preproj.fc1.bias\", \"decoder.decoder.dit.preproj.fc2.weight\", \"decoder.decoder.dit.preproj.fc2.bias\", \"decoder.decoder.dit.t_embedder.mlp.0.weight\", \"decoder.decoder.dit.t_embedder.mlp.0.bias\", \"decoder.decoder.dit.t_embedder.mlp.2.weight\", \"decoder.decoder.dit.t_embedder.mlp.2.bias\", \"decoder.decoder.dit.blocks.0.norm1.weight\", \"decoder.decoder.dit.blocks.0.norm1.bias\", \"decoder.decoder.dit.blocks.0.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.0.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.0.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.0.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.0.norm2.weight\", \"decoder.decoder.dit.blocks.0.norm2.bias\", \"decoder.decoder.dit.blocks.0.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.0.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.0.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.0.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.0.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.0.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.0.norm3.weight\", \"decoder.decoder.dit.blocks.0.norm3.bias\", \"decoder.decoder.dit.blocks.0.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.0.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.0.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.0.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.0.norm4.weight\", \"decoder.decoder.dit.blocks.0.norm4.bias\", \"decoder.decoder.dit.blocks.0.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.0.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.0.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.0.mlp2.fc2.bias\", \"decoder.decoder.dit.blocks.1.norm1.weight\", \"decoder.decoder.dit.blocks.1.norm1.bias\", \"decoder.decoder.dit.blocks.1.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.1.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.1.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.1.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.1.norm2.weight\", \"decoder.decoder.dit.blocks.1.norm2.bias\", \"decoder.decoder.dit.blocks.1.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.1.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.1.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.1.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.1.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.1.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.1.norm3.weight\", \"decoder.decoder.dit.blocks.1.norm3.bias\", \"decoder.decoder.dit.blocks.1.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.1.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.1.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.1.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.1.norm4.weight\", \"decoder.decoder.dit.blocks.1.norm4.bias\", \"decoder.decoder.dit.blocks.1.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.1.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.1.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.1.mlp2.fc2.bias\", \"decoder.decoder.dit.blocks.2.norm1.weight\", \"decoder.decoder.dit.blocks.2.norm1.bias\", \"decoder.decoder.dit.blocks.2.attn.in_proj_weight\", \"decoder.decoder.dit.blocks.2.attn.in_proj_bias\", \"decoder.decoder.dit.blocks.2.attn.out_proj.weight\", \"decoder.decoder.dit.blocks.2.attn.out_proj.bias\", \"decoder.decoder.dit.blocks.2.norm2.weight\", \"decoder.decoder.dit.blocks.2.norm2.bias\", \"decoder.decoder.dit.blocks.2.mlp1.fc1.weight\", \"decoder.decoder.dit.blocks.2.mlp1.fc1.bias\", \"decoder.decoder.dit.blocks.2.mlp1.fc2.weight\", \"decoder.decoder.dit.blocks.2.mlp1.fc2.bias\", \"decoder.decoder.dit.blocks.2.adaLN_modulation.1.weight\", \"decoder.decoder.dit.blocks.2.adaLN_modulation.1.bias\", \"decoder.decoder.dit.blocks.2.norm3.weight\", \"decoder.decoder.dit.blocks.2.norm3.bias\", \"decoder.decoder.dit.blocks.2.cross_attn.in_proj_weight\", \"decoder.decoder.dit.blocks.2.cross_attn.in_proj_bias\", \"decoder.decoder.dit.blocks.2.cross_attn.out_proj.weight\", \"decoder.decoder.dit.blocks.2.cross_attn.out_proj.bias\", \"decoder.decoder.dit.blocks.2.norm4.weight\", \"decoder.decoder.dit.blocks.2.norm4.bias\", \"decoder.decoder.dit.blocks.2.mlp2.fc1.weight\", \"decoder.decoder.dit.blocks.2.mlp2.fc1.bias\", \"decoder.decoder.dit.blocks.2.mlp2.fc2.weight\", \"decoder.decoder.dit.blocks.2.mlp2.fc2.bias\", \"decoder.decoder.dit.final_layer.norm_final.weight\", \"decoder.decoder.dit.final_layer.norm_final.bias\", \"decoder.decoder.dit.final_layer.proj.0.weight\", \"decoder.decoder.dit.final_layer.proj.0.bias\", \"decoder.decoder.dit.final_layer.proj.1.weight\", \"decoder.decoder.dit.final_layer.proj.1.bias\", \"decoder.decoder.dit.final_layer.proj.3.weight\", \"decoder.decoder.dit.final_layer.proj.3.bias\", \"decoder.decoder.dit.final_layer.proj.4.weight\", \"decoder.decoder.dit.final_layer.proj.4.bias\", \"decoder.decoder.dit.final_layer.adaLN_modulation.1.weight\", \"decoder.decoder.dit.final_layer.adaLN_modulation.1.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "# ====================== 1. 配置类（完全复刻 args.json 中的训练参数） ======================\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # 从 args.json 复制的核心参数（一字不差）\n",
    "        self.future_len = 80              # 未来轨迹长度（可能对应扩散步数）\n",
    "        self.time_len = 21                # 时间步长度（如历史+未来）\n",
    "        self.agent_state_dim = 11         # 智能体状态维度（如x,y,vx,vy等）\n",
    "        self.agent_num = 32               # 最大智能体数量（输入中智能体的个数）\n",
    "        self.static_objects_state_dim = 10# 静态物体状态维度\n",
    "        self.static_objects_num = 5       # 静态物体数量\n",
    "        self.lane_len = 20                # 单条车道的特征长度\n",
    "        self.lane_state_dim = 12          # 车道特征维度\n",
    "        self.lane_num = 70                # 最大车道数量\n",
    "        self.map_len = 10                 # 地图特征长度\n",
    "        self.map_state_dim = 4            # 地图特征维度\n",
    "        self.map_num = 5                  # 地图特征数量\n",
    "        self.route_len = 20               # 路径特征长度\n",
    "        self.route_state_dim = 12         # 路径特征维度\n",
    "        self.route_num = 25               # 路径点数量\n",
    "        self.encoder_drop_path_rate = 0.1 # 编码器DropPath概率\n",
    "        self.decoder_drop_path_rate = 0.1 # 解码器DropPath概率\n",
    "        self.encoder_depth = 3            # 编码器层数（Transformer/DiT）\n",
    "        self.decoder_depth = 3            # 解码器层数（Transformer/DiT）\n",
    "        self.num_heads = 6                # 注意力头数\n",
    "        self.hidden_dim = 192             # 隐藏层维度（模型核心维度）\n",
    "        self.diffusion_model_type = \"x_start\" # 扩散模型类型（从x_start开始扩散）\n",
    "        self.predicted_neighbor_num = 10  # 预测的邻居车辆数量\n",
    "        self.output_dim = 4               # 每条轨迹的维度（x,y,vx,vy，默认4维）\n",
    "\n",
    "\n",
    "# ====================== 2. 模型类（基于 args.json 参数构建，与训练一致） ======================\n",
    "class Diffusion_Planner(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # -------------------------- 核心结构：基于 args.json 参数构建 --------------------------\n",
    "        # 1. 输入嵌入层（将各模态特征映射到 hidden_dim）\n",
    "        self.agent_emb = nn.Linear(config.agent_state_dim, config.hidden_dim)  # 智能体特征嵌入\n",
    "        self.lane_emb = nn.Linear(config.lane_state_dim, config.hidden_dim)    # 车道特征嵌入\n",
    "        self.static_emb = nn.Linear(config.static_objects_state_dim, config.hidden_dim)  # 静态物体嵌入\n",
    "        self.route_emb = nn.Linear(config.route_state_dim, config.hidden_dim)  # 路径特征嵌入\n",
    "        \n",
    "        # 2. 编码器（Transformer/DiT，层数=encoder_depth，头数=num_heads）\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.hidden_dim,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=config.hidden_dim * 4,  # 常规设置：4倍hidden_dim\n",
    "            dropout=config.encoder_drop_path_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.encoder_depth)\n",
    "        \n",
    "        # 3. 解码器（Transformer/DiT，层数=decoder_depth，头数=num_heads）\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=config.hidden_dim,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=config.hidden_dim * 4,\n",
    "            dropout=config.decoder_drop_path_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=config.decoder_depth)\n",
    "        \n",
    "        # 4. 输出层（预测未来轨迹：未来长度×输出维度）\n",
    "        self.output_layer = nn.Linear(\n",
    "            config.hidden_dim,\n",
    "            config.future_len * config.output_dim  # 输出：future_len步 × 4维轨迹\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]):\n",
    "        \"\"\"前向传播（与训练时逻辑一致，基于扩散模型类型\"x_start\"）\"\"\"\n",
    "        # -------------------------- 1. 提取输入特征（与npz键名对应） --------------------------\n",
    "        agent_states = inputs[\"agent_states\"]       # 形状：(batch, agent_num, agent_state_dim)\n",
    "        lane_states = inputs[\"lane_states\"]         # 形状：(batch, lane_num, lane_state_dim)\n",
    "        static_states = inputs[\"static_states\"]     # 形状：(batch, static_objects_num, static_objects_state_dim)\n",
    "        route_states = inputs[\"route_states\"]       # 形状：(batch, route_num, route_state_dim)\n",
    "        \n",
    "        batch_size = agent_states.shape[0]\n",
    "        \n",
    "        # -------------------------- 2. 输入嵌入（各模态特征映射到hidden_dim） --------------------------\n",
    "        agent_emb = self.agent_emb(agent_states)    # (batch, 32, 192)\n",
    "        lane_emb = self.lane_emb(lane_states)        # (batch, 70, 192)\n",
    "        static_emb = self.static_emb(static_states)  # (batch, 5, 192)\n",
    "        route_emb = self.route_emb(route_states)    # (batch, 25, 192)\n",
    "        \n",
    "        # 拼接所有条件特征（与训练时的融合方式一致）\n",
    "        cond_feat = torch.cat([agent_emb, lane_emb, static_emb, route_emb], dim=1)  # (batch, 32+70+5+25=132, 192)\n",
    "        \n",
    "        # -------------------------- 3. 编码+解码（扩散模型的核心前向逻辑） --------------------------\n",
    "        # 编码器处理条件特征\n",
    "        enc_out = self.encoder(cond_feat)  # (batch, 132, 192)\n",
    "        \n",
    "        # 解码器输入：扩散过程的噪声轨迹（此处简化，与训练时一致即可）\n",
    "        # 假设解码器输入是\"x_start\"（扩散模型类型）对应的初始轨迹嵌入\n",
    "        dec_in = torch.zeros(batch_size, self.config.future_len, self.config.hidden_dim, device=agent_states.device)\n",
    "        dec_out = self.decoder(dec_in, enc_out)  # (batch, 80, 192)\n",
    "        \n",
    "        # -------------------------- 4. 输出轨迹预测 --------------------------\n",
    "        pred_flat = self.output_layer(dec_out)  # (batch, 80, 4)\n",
    "        # 调整为训练时的输出格式：(batch, 1, future_len, output_dim)\n",
    "        prediction = pred_flat.unsqueeze(1)  # (batch, 1, 80, 4)\n",
    "        \n",
    "        return None, {\"prediction\": prediction}  # 保持与训练时的输出格式一致\n",
    "\n",
    "\n",
    "# ====================== 3. 数据处理器（仅格式转换，匹配 args.json 数据维度） ======================\n",
    "class DataProcessor:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "    def observation_adapter(self, npz_data: dict, device: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        仅将npz中的numpy数组转为Tensor，维度与 args.json 完全匹配\n",
    "        键名必须与npz文件中的键一致（确保能读取到预处理好的数据）\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # 智能体状态：(batch, agent_num=32, agent_state_dim=11)\n",
    "            \"agent_states\": torch.tensor(npz_data[\"agent_states\"], dtype=torch.float32).to(device),\n",
    "            # 车道状态：(batch, lane_num=70, lane_state_dim=12)\n",
    "            \"lane_states\": torch.tensor(npz_data[\"lane_states\"], dtype=torch.float32).to(device),\n",
    "            # 静态物体状态：(batch, static_objects_num=5, static_objects_state_dim=10)\n",
    "            \"static_states\": torch.tensor(npz_data[\"static_states\"], dtype=torch.float32).to(device),\n",
    "            # 路径状态：(batch, route_num=25, route_state_dim=12)\n",
    "            \"route_states\": torch.tensor(npz_data[\"route_states\"], dtype=torch.float32).to(device),\n",
    "            # 若npz中还有其他键（如\"map_states\"），根据 args.json 的map参数补充\n",
    "        }\n",
    "\n",
    "\n",
    "# ====================== 4. 模型加载与推理工具 ======================\n",
    "def load_training_model(ckpt_path: str, config: Config, device: str = \"cpu\") -> Diffusion_Planner:\n",
    "    \"\"\"加载训练好的模型，处理EMA权重和多卡前缀\"\"\"\n",
    "    model = Diffusion_Planner(config)\n",
    "    state_dict = torch.load(ckpt_path, map_location=device)\n",
    "    \n",
    "    # 处理EMA权重（训练时可能保存了EMA权重）\n",
    "    if \"ema_state_dict\" in state_dict:\n",
    "        state_dict = state_dict[\"ema_state_dict\"]\n",
    "    # 处理模型权重（训练时可能用\"model\"键封装）\n",
    "    elif \"model\" in state_dict:\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    \n",
    "    # 处理多卡训练的\"module.\"前缀（若训练时用了DataParallel/DistributedDataParallel）\n",
    "    model_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"module.\"):\n",
    "            model_state_dict[k[len(\"module.\"):]] = v  # 移除\"module.\"\n",
    "        else:\n",
    "            model_state_dict[k] = v\n",
    "    \n",
    "    # 加载权重（确保模型结构与训练完全一致，否则会报错）\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()  # 切换到推理模式（关闭Dropout等训练特有的层）\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def infer_preprocessed_data(model: Diffusion_Planner, npz_path: str, device: str) -> np.ndarray:\n",
    "    \"\"\"对预处理好的npz数据执行推理，返回最终轨迹预测\"\"\"\n",
    "    # 1. 加载npz数据（已预处理，维度与 args.json 匹配）\n",
    "    npz_data = np.load(npz_path)\n",
    "    \n",
    "    # 2. 格式转换（numpy → Tensor，无额外处理）\n",
    "    processor = DataProcessor(model.config)\n",
    "    model_inputs = processor.observation_adapter(npz_data, device)\n",
    "    \n",
    "    # 3. 推理（关闭梯度计算，加速且避免内存占用）\n",
    "    with torch.no_grad():\n",
    "        _, outputs = model(model_inputs)\n",
    "    \n",
    "    # 4. 提取预测结果（格式：(future_len=80, output_dim=4)）\n",
    "    # 训练时输出格式：(batch, 1, 80, 4)，推理时batch=1，取第一个样本\n",
    "    predictions = outputs[\"prediction\"][0, 0].detach().cpu().numpy().astype(np.float64)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def batch_infer(model: Diffusion_Planner, data_dir: str, device: str):\n",
    "    \"\"\"批量推理文件夹中的所有预处理npz文件\"\"\"\n",
    "    print(f\"开始批量推理，共 {len([f for f in os.listdir(data_dir) if f.endswith('.npz')])} 个文件\")\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".npz\"):\n",
    "            npz_path = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                pred_trajectory = infer_preprocessed_data(model, npz_path, device)\n",
    "                print(f\"✅ {filename} | 预测轨迹形状: {pred_trajectory.shape}（{pred_trajectory.shape[0]}步 × {pred_trajectory.shape[1]}维）\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {filename} | 推理失败: {str(e)}\")\n",
    "    print(\"批量推理完成！\")\n",
    "\n",
    "\n",
    "# ====================== 5. 执行入口（直接运行） ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # -------------------------- 推理配置（仅需调整路径，参数已与训练对齐） --------------------------\n",
    "    DEVICE = \"cpu\"  # 若有GPU可改为\"cuda\"（与训练时的device兼容）\n",
    "    CKPT_PATH = \"model.pth\"  # 训练好的模型权重路径\n",
    "    DATA_DIR = \"diffusion_planner_devset\"  # 预处理好的npz数据文件夹\n",
    "\n",
    "    # -------------------------- 执行推理 --------------------------\n",
    "    print(\"1. 初始化训练配置（与 args.json 完全一致）...\")\n",
    "    config = Config()\n",
    "\n",
    "    print(\"2. 加载训练好的模型...\")\n",
    "    model = load_training_model(CKPT_PATH, config, DEVICE)\n",
    "\n",
    "    print(\"3. 开始批量推理...\")\n",
    "    batch_infer(model, DATA_DIR, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0953180d-1061-4ab1-bbbf-b9b01093b527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 加载模型（复用训练时的原结构）...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Config' has no attribute 'from_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# 执行推理\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. 加载模型（复用训练时的原结构）...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m model, config \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCKPT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. 批量推理...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m batch_infer(model, config, DATA_DIR, DEVICE)\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(ckpt_path, config_path, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"加载清理后的原模型，处理EMA和多卡权重\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 加载训练时的配置（args.json）\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m(config_path)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 初始化原模型（结构与训练完全一致）\u001b[39;00m\n\u001b[0;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m Diffusion_Planner(config)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Config' has no attribute 'from_file'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict\n",
    "\n",
    "# 1. 将项目根目录加入 Python 路径（确保能导入 diffusion_planner）\n",
    "# sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "# 2. 直接导入项目中清理后的原模型类（核心！与训练结构完全一致）\n",
    "from diffusion_planner.model.diffusion_planner import Diffusion_Planner  # 复用训练时的模型类\n",
    "# from diffusion_planner.data_process.data_processor import DataProcessor  # 复用项目数据处理器（已清理nuplan）\n",
    "from diffusion_planner.utils.config import Config  # 复用项目配置类（从 args.json 加载）\n",
    "\n",
    "\n",
    "# ====================== 模型加载（复用训练权重逻辑） ======================\n",
    "def load_model(ckpt_path: str, config_path: str, device: str = \"cpu\") -> (Diffusion_Planner, Config):\n",
    "    \"\"\"加载清理后的原模型，处理EMA和多卡权重\"\"\"\n",
    "    # 加载训练时的配置（args.json）\n",
    "    config = Config.from_file(config_path)\n",
    "    # 初始化原模型（结构与训练完全一致）\n",
    "    model = Diffusion_Planner(config)\n",
    "    # 加载权重\n",
    "    state_dict = torch.load(ckpt_path, map_location=device)\n",
    "    \n",
    "    # 处理 EMA 权重（与训练时保存逻辑一致）\n",
    "    if \"ema_state_dict\" in state_dict:\n",
    "        state_dict = state_dict[\"ema_state_dict\"]\n",
    "    elif \"model\" in state_dict:\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    \n",
    "    # 处理多卡训练的 \"module.\" 前缀\n",
    "    model_state_dict = {\n",
    "        k[len(\"module.\"): ] if k.startswith(\"module.\") else k: v \n",
    "        for k, v in state_dict.items()\n",
    "    }\n",
    "    \n",
    "    # 加载权重（此时结构完全一致，无键不匹配错误）\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    return model.to(device), config\n",
    "\n",
    "\n",
    "# ====================== 推理逻辑（数据已预处理，仅格式转换） ======================\n",
    "def infer_single(model: Diffusion_Planner, config: Config, npz_path: str, device: str) -> np.ndarray:\n",
    "    \"\"\"对单个预处理npz文件推理\"\"\"\n",
    "    # 1. 加载npz数据（键名与模型输入匹配）\n",
    "    npz_data = np.load(npz_path)\n",
    "    # 2. 格式转换（numpy→Tensor，用项目数据处理器）\n",
    "    processor = DataProcessor(config)\n",
    "    model_inputs = processor.observation_adapter(\n",
    "        npz_data=npz_data,\n",
    "        device=device\n",
    "    )\n",
    "    # 3. 推理（关闭梯度）\n",
    "    with torch.no_grad():\n",
    "        _, outputs = model(model_inputs)\n",
    "    # 4. 提取结果（与训练时输出格式一致）\n",
    "    return outputs[\"prediction\"][0, 0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def batch_infer(model: Diffusion_Planner, config: Config, data_dir: str, device: str):\n",
    "    \"\"\"批量推理所有npz文件\"\"\"\n",
    "    file_count = len([f for f in os.listdir(data_dir) if f.endswith(\".npz\")])\n",
    "    print(f\"发现 {file_count} 个预处理文件，开始推理...\")\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".npz\"):\n",
    "            npz_path = os.path.join(data_dir, filename)\n",
    "            try:\n",
    "                pred = infer_single(model, config, npz_path, device)\n",
    "                print(f\"✅ {filename} | 轨迹形状: {pred.shape}（{pred.shape[0]}步×{pred.shape[1]}维）\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {filename} | 错误: {str(e)}\")\n",
    "\n",
    "\n",
    "# ====================== 执行入口 ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置路径（根据你的项目实际路径调整）\n",
    "    DEVICE = \"cpu\"\n",
    "    CKPT_PATH = \"model.pth\"          # 训练权重路径\n",
    "    CONFIG_PATH = \"args.json\"        # 训练配置文件（args.json）\n",
    "    DATA_DIR = \"diffusion_planner_devset\"  # 预处理数据文件夹\n",
    "\n",
    "    # 执行推理\n",
    "    print(\"1. 加载模型（复用训练时的原结构）...\")\n",
    "    model, config = load_model(CKPT_PATH, CONFIG_PATH, DEVICE)\n",
    "    \n",
    "    print(\"2. 批量推理...\")\n",
    "    batch_infer(model, config, DATA_DIR, DEVICE)\n",
    "    print(\"推理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b4f7d8-5509-44c4-bf83-7599c207e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 加载模型（手动配置+原训练结构）...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# 执行推理（核心修改：用手动加载配置替代 from_file）\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. 加载模型（手动配置+原训练结构）...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m model, config \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCKPT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG_JSON_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. 开始批量推理...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m batch_infer(model, config, DATA_DIR, DEVICE)\n",
      "Cell \u001b[1;32mIn[8], line 63\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(ckpt_path, config_json_path, device)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config_from_json(config_json_path)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 2. 初始化原模型（结构与训练完全一致，用手动构建的 config 传参）\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusion_Planner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# 3. 加载权重（处理 EMA 和多卡前缀，逻辑不变）\u001b[39;00m\n\u001b[0;32m     66\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mD:\\CUP\\Foton_intership\\nuplan_project\\Diffusion-Planner-main\\diffusion_planner\\model\\diffusion_planner.py:13\u001b[0m, in \u001b[0;36mDiffusion_Planner.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusion_Planner_Encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m Diffusion_Planner_Decoder(config)\n",
      "File \u001b[1;32mD:\\CUP\\Foton_intership\\nuplan_project\\Diffusion-Planner-main\\diffusion_planner\\model\\diffusion_planner.py:32\u001b[0m, in \u001b[0;36mDiffusion_Planner_Encoder.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_weights()\n",
      "File \u001b[1;32mD:\\CUP\\Foton_intership\\nuplan_project\\Diffusion-Planner-main\\diffusion_planner\\model\\module\\encoder.py:26\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_encoder \u001b[38;5;241m=\u001b[39m StaticFusionEncoder(config\u001b[38;5;241m.\u001b[39mstatic_objects_state_dim, drop_path_rate\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoder_drop_path_rate, hidden_dim\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_encoder \u001b[38;5;241m=\u001b[39m LaneFusionEncoder(config\u001b[38;5;241m.\u001b[39mlane_len, drop_path_rate\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoder_drop_path_rate, hidden_dim\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_dim, depth\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoder_depth)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusion \u001b[38;5;241m=\u001b[39m FusionEncoder(\n\u001b[0;32m     22\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_dim, \n\u001b[0;32m     23\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_heads, \n\u001b[0;32m     24\u001b[0m     drop_path_rate\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoder_drop_path_rate, \n\u001b[0;32m     25\u001b[0m     depth\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoder_depth, \n\u001b[1;32m---> 26\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# position embedding encode x, y, cos, sin, type\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m7\u001b[39m, config\u001b[38;5;241m.\u001b[39mhidden_dim)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Config' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json  # 用于读取 args.json 文件\n",
    "from typing import Dict\n",
    "\n",
    "# 1. 将项目根目录加入路径（确保能导入 diffusion_planner）\n",
    "# sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "# 2. 导入项目中清理后的核心模块（无 nuplan 依赖）\n",
    "from diffusion_planner.model.diffusion_planner import Diffusion_Planner  # 原训练模型类\n",
    "# from diffusion_planner.data_process.data_processor import DataProcessor  # 原数据处理器\n",
    "\n",
    "\n",
    "# ====================== 手动构建 Config 实例（绕开 from_file 方法） ======================\n",
    "def load_config_from_json(json_path: str) -> object:\n",
    "    \"\"\"从 args.json 读取参数，手动构建 Config 实例（与训练时参数完全一致）\"\"\"\n",
    "    # 读取 args.json 中的训练参数\n",
    "    with open(json_path, \"r\") as f:\n",
    "        config_dict = json.load(f)\n",
    "    \n",
    "    # 手动创建 Config 类（属性与原类完全一致，匹配模型初始化需求）\n",
    "    class Config:\n",
    "        def __init__(self, params):\n",
    "            # 从 args.json 复制所有参数（确保键名与原 Config 类属性一致）\n",
    "            self.future_len = params[\"future_len\"]\n",
    "            self.time_len = params[\"time_len\"]\n",
    "            self.agent_state_dim = params[\"agent_state_dim\"]\n",
    "            self.agent_num = params[\"agent_num\"]\n",
    "            self.static_objects_state_dim = params[\"static_objects_state_dim\"]\n",
    "            self.static_objects_num = params[\"static_objects_num\"]\n",
    "            self.lane_len = params[\"lane_len\"]\n",
    "            self.lane_state_dim = params[\"lane_state_dim\"]\n",
    "            self.lane_num = params[\"lane_num\"]\n",
    "            self.map_len = params[\"map_len\"]\n",
    "            self.map_state_dim = params[\"map_state_dim\"]\n",
    "            self.map_num = params[\"map_num\"]\n",
    "            self.route_len = params[\"route_len\"]\n",
    "            self.route_state_dim = params[\"route_state_dim\"]\n",
    "            self.route_num = params[\"route_num\"]\n",
    "            self.encoder_drop_path_rate = params[\"encoder_drop_path_rate\"]\n",
    "            self.decoder_drop_path_rate = params[\"decoder_drop_path_rate\"]\n",
    "            self.encoder_depth = params[\"encoder_depth\"]\n",
    "            self.decoder_depth = params[\"decoder_depth\"]\n",
    "            self.num_heads = params[\"num_heads\"]\n",
    "            self.hidden_dim = params[\"hidden_dim\"]\n",
    "            self.diffusion_model_type = params[\"diffusion_model_type\"]\n",
    "            self.predicted_neighbor_num = params[\"predicted_neighbor_num\"]\n",
    "            self.output_dim = params.get(\"output_dim\", 4)  # 默认4维（x,y,vx,vy）\n",
    "    \n",
    "    # 返回手动构建的 Config 实例（参数与训练完全一致）\n",
    "    return Config(config_dict)\n",
    "\n",
    "\n",
    "# ====================== 模型加载（手动传参，无 from_file 依赖） ======================\n",
    "def load_model(ckpt_path: str, config_json_path: str, device: str = \"cpu\") -> (Diffusion_Planner, object):\n",
    "    \"\"\"加载原模型+手动配置，绕开 from_file 方法\"\"\"\n",
    "    # 1. 手动加载配置（从 args.json 读取，无 from_file 依赖）\n",
    "    config = load_config_from_json(config_json_path)\n",
    "    \n",
    "    # 2. 初始化原模型（结构与训练完全一致，用手动构建的 config 传参）\n",
    "    model = Diffusion_Planner(config)\n",
    "    \n",
    "    # 3. 加载权重（处理 EMA 和多卡前缀，逻辑不变）\n",
    "    state_dict = torch.load(ckpt_path, map_location=device)\n",
    "    # 处理 EMA 权重\n",
    "    if \"ema_state_dict\" in state_dict:\n",
    "        state_dict = state_dict[\"ema_state_dict\"]\n",
    "    elif \"model\" in state_dict:\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    # 处理多卡 \"module.\" 前缀\n",
    "    model_state_dict = {\n",
    "        k[len(\"module.\"): ] if k.startswith(\"module.\") else k: v \n",
    "        for k, v in state_dict.items()\n",
    "    }\n",
    "    # 加载权重（此时配置参数与训练一致，模型结构匹配）\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    return model.to(device), config\n",
    "\n",
    "\n",
    "# ====================== 推理逻辑（无修改，确保数据格式匹配） ======================\n",
    "def infer_single(model: Diffusion_Planner, config: object, npz_path: str, device: str) -> np.ndarray:\n",
    "    \"\"\"对单个预处理npz文件推理\"\"\"\n",
    "    # 1. 加载npz数据（键名需与 DataProcessor 期望的一致）\n",
    "    npz_data = np.load(npz_path)\n",
    "    # 2. 格式转换（用项目原 DataProcessor，确保与训练输入一致）\n",
    "    processor = DataProcessor(config)  # 传入手动构建的 config\n",
    "    # 注意：npz的键名（如\"agent_states\"）需与 processor.observation_adapter 接收的参数一致\n",
    "    model_inputs = processor.observation_adapter(\n",
    "        history=npz_data.get(\"history\", npz_data),  # 若npz直接存特征，可调整\n",
    "        traffic_light_data=npz_data.get(\"traffic_light\", np.array([])),\n",
    "        device=device\n",
    "    )\n",
    "    # 3. 推理（关闭梯度）\n",
    "    with torch.no_grad():\n",
    "        _, outputs = model(model_inputs)\n",
    "    # 4. 提取结果（与训练输出格式一致：(future_len, output_dim)）\n",
    "    return outputs[\"prediction\"][0, 0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def batch_infer(model: Diffusion_Planner, config: object, data_dir: str, device: str):\n",
    "    \"\"\"批量推理所有npz文件\"\"\"\n",
    "    file_list = [f for f in os.listdir(data_dir) if f.endswith(\".npz\")]\n",
    "    print(f\"发现 {len(file_list)} 个预处理文件，开始推理...\")\n",
    "    \n",
    "    for filename in file_list:\n",
    "        npz_path = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            pred = infer_single(model, config, npz_path, device)\n",
    "            print(f\"✅ {filename} | 轨迹形状: {pred.shape}（{pred.shape[0]}步×{pred.shape[1]}维）\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {filename} | 错误: {str(e)}\")\n",
    "\n",
    "\n",
    "# ====================== 执行入口 ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置路径（根据你的项目实际路径调整）\n",
    "    DEVICE = \"cpu\"\n",
    "    CKPT_PATH = \"model.pth\"          # 训练权重路径\n",
    "    CONFIG_JSON_PATH = \"args.json\"    # 训练参数文件（args.json）\n",
    "    DATA_DIR = \"diffusion_planner_devset\"  # 预处理数据文件夹\n",
    "\n",
    "    # 执行推理（核心修改：用手动加载配置替代 from_file）\n",
    "    print(\"1. 加载模型（手动配置+原训练结构）...\")\n",
    "    model, config = load_model(CKPT_PATH, CONFIG_JSON_PATH, DEVICE)\n",
    "    \n",
    "    print(\"2. 开始批量推理...\")\n",
    "    batch_infer(model, config, DATA_DIR, DEVICE)\n",
    "    print(\"推理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0523a56a-dbe8-4404-9789-4f7ac78aec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Diffusion Planner Inference Pipeline\n",
      "============================================================\n",
      "\n",
      "[1/4] Loading arguments from args.json...\n",
      "  Loaded 26 parameters\n",
      "\n",
      "[2/4] Creating dataset from diffusion_planner_devset...\n",
      "Found 21 data files\n",
      "  Dataset size: 21 samples\n",
      "  Number of batches: 21\n",
      "\n",
      "  Sample data structure:\n",
      "    map_name: shape=(), dtype=<U12 (numpy)\n",
      "    token: shape=(), dtype=<U16 (numpy)\n",
      "    ego_current_state: shape=torch.Size([10]), dtype=torch.float32\n",
      "    ego_agent_future: shape=torch.Size([80, 3]), dtype=torch.float32\n",
      "    neighbor_agents_past: shape=torch.Size([32, 21, 11]), dtype=torch.float32\n",
      "    neighbor_agents_future: shape=torch.Size([32, 80, 3]), dtype=torch.float32\n",
      "    static_objects: shape=torch.Size([5, 10]), dtype=torch.float32\n",
      "    lanes: shape=torch.Size([70, 20, 12]), dtype=torch.float32\n",
      "    lanes_speed_limit: shape=torch.Size([70, 1]), dtype=torch.float32\n",
      "    lanes_has_speed_limit: shape=torch.Size([70, 1]), dtype=torch.bool\n",
      "    route_lanes: shape=torch.Size([25, 20, 12]), dtype=torch.float32\n",
      "    route_lanes_speed_limit: shape=torch.Size([25, 1]), dtype=torch.float32\n",
      "    route_lanes_has_speed_limit: shape=torch.Size([25, 1]), dtype=torch.bool\n",
      "\n",
      "[3/4] Loading model from model.pth...\n",
      "Checkpoint loaded from model.pth\n",
      "Checkpoint keys: dict_keys(['model', 'ema_state_dict'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 301\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 301\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 276\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# 加载模型\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[3/4] Loading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 276\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# 执行推理\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[4/4] Running inference...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 84\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_path, device, use_ema)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# 提取模型\u001b[39;00m\n\u001b[0;32m     83\u001b[0m model \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 84\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[0;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# 如果使用EMA模型\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "\n",
    "# 自定义数据集类\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        加载预处理好的数据\n",
    "        Args:\n",
    "            data_dir: 包含.npz文件的目录路径\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_files = sorted(list(self.data_dir.glob('*.npz')))\n",
    "        \n",
    "        if len(self.data_files) == 0:\n",
    "            raise ValueError(f\"No .npz files found in {data_dir}\")\n",
    "        \n",
    "        print(f\"Found {len(self.data_files)} data files\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        加载单个.npz文件\n",
    "        返回的数据应该包含模型需要的所有输入\n",
    "        \"\"\"\n",
    "        data_file = self.data_files[idx]\n",
    "        data = np.load(data_file, allow_pickle=True)\n",
    "        \n",
    "        # 将numpy数组转换为torch tensor，但保留字符串等特殊类型\n",
    "        sample = {}\n",
    "        for key in data.files:\n",
    "            value = data[key]\n",
    "            \n",
    "            # 检查数据类型\n",
    "            if isinstance(value, np.ndarray):\n",
    "                # 如果是数值类型，转换为tensor\n",
    "                if np.issubdtype(value.dtype, np.number):\n",
    "                    sample[key] = torch.from_numpy(value).float()\n",
    "                # 如果是布尔类型\n",
    "                elif np.issubdtype(value.dtype, np.bool_):\n",
    "                    sample[key] = torch.from_numpy(value)\n",
    "                # 如果是字符串或对象类型，保持原样\n",
    "                else:\n",
    "                    sample[key] = value\n",
    "            else:\n",
    "                sample[key] = value\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "def load_args(args_path='args.json'):\n",
    "    \"\"\"加载参数配置文件\"\"\"\n",
    "    with open(args_path, 'r') as f:\n",
    "        args_dict = json.load(f)\n",
    "    \n",
    "    # 将字典转换为argparse.Namespace对象\n",
    "    args = argparse.Namespace(**args_dict)\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_model(model_path, device='cpu', use_ema=True):\n",
    "    \"\"\"\n",
    "    加载训练好的模型\n",
    "    Args:\n",
    "        model_path: 模型文件路径\n",
    "        device: 运行设备\n",
    "        use_ema: 是否使用EMA模型（通常EMA模型效果更好）\n",
    "    \"\"\"\n",
    "    # 加载检查点\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    print(f\"Checkpoint loaded from {model_path}\")\n",
    "    print(f\"Checkpoint keys: {checkpoint.keys()}\")\n",
    "    \n",
    "    # 提取模型\n",
    "    model = checkpoint['model']\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 如果使用EMA模型\n",
    "    if use_ema and 'ema_state_dict' in checkpoint:\n",
    "        print(\"Loading EMA state dict...\")\n",
    "        # EMA state dict 需要加载到模型中\n",
    "        if hasattr(model, 'load_state_dict'):\n",
    "            try:\n",
    "                model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "                print(\"EMA model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load EMA state dict: {e}\")\n",
    "                print(\"Using original model instead\")\n",
    "    \n",
    "    print(f\"Model type: {type(model)}\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_batch_for_model(batch, device):\n",
    "    \"\"\"\n",
    "    准备batch数据用于模型输入\n",
    "    只将tensor类型的数据移到device，保留其他类型\n",
    "    \"\"\"\n",
    "    batch_device = {}\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            batch_device[key] = value.to(device)\n",
    "        else:\n",
    "            batch_device[key] = value\n",
    "    return batch_device\n",
    "\n",
    "\n",
    "def inference(model, dataloader, device='cpu', save_results=True, output_dir='inference_results'):\n",
    "    \"\"\"\n",
    "    执行推理\n",
    "    Args:\n",
    "        model: 加载的模型\n",
    "        dataloader: 数据加载器\n",
    "        device: 运行设备\n",
    "        save_results: 是否保存结果\n",
    "        output_dir: 结果保存目录\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if save_results:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(\"\\nStarting inference...\")\n",
    "    print(f\"Model methods: {[m for m in dir(model) if not m.startswith('_') and callable(getattr(model, m))][:15]}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # 将数据移到指定设备（只移动tensor）\n",
    "            batch_device = prepare_batch_for_model(batch, device)\n",
    "            \n",
    "            try:\n",
    "                # 执行推理 - 根据第一张图的代码，diffusion_planner可能有sample方法\n",
    "                if hasattr(model, 'conditional_sample'):\n",
    "                    # 扩散模型的条件采样\n",
    "                    predictions = model.conditional_sample(batch_device)\n",
    "                elif hasattr(model, 'sample'):\n",
    "                    predictions = model.sample(batch_device)\n",
    "                elif hasattr(model, 'predict'):\n",
    "                    predictions = model.predict(batch_device)\n",
    "                elif hasattr(model, 'forward'):\n",
    "                    predictions = model(batch_device)\n",
    "                else:\n",
    "                    raise AttributeError(f\"Model has no recognized inference method\")\n",
    "                \n",
    "                # 处理预测结果\n",
    "                if isinstance(predictions, torch.Tensor):\n",
    "                    result = predictions.cpu().numpy()\n",
    "                elif isinstance(predictions, dict):\n",
    "                    result = {k: v.cpu().numpy() if isinstance(v, torch.Tensor) else v \n",
    "                             for k, v in predictions.items()}\n",
    "                elif isinstance(predictions, (list, tuple)):\n",
    "                    result = [v.cpu().numpy() if isinstance(v, torch.Tensor) else v \n",
    "                             for v in predictions]\n",
    "                else:\n",
    "                    result = predictions\n",
    "                \n",
    "                # 记录结果\n",
    "                result_entry = {\n",
    "                    'batch_idx': batch_idx,\n",
    "                    'file_name': dataloader.dataset.data_files[batch_idx].name,\n",
    "                    'predictions': result\n",
    "                }\n",
    "                all_results.append(result_entry)\n",
    "                \n",
    "                # 保存单个批次结果\n",
    "                if save_results:\n",
    "                    output_file = os.path.join(output_dir, \n",
    "                                              f'result_{dataloader.dataset.data_files[batch_idx].stem}.npz')\n",
    "                    if isinstance(result, dict):\n",
    "                        np.savez(output_file, **result)\n",
    "                    elif isinstance(result, (list, tuple)):\n",
    "                        np.savez(output_file, *result)\n",
    "                    else:\n",
    "                        np.savez(output_file, predictions=result)\n",
    "                \n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"Processed {batch_idx + 1}/{len(dataloader)} batches\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing batch {batch_idx} (file: {dataloader.dataset.data_files[batch_idx].name}):\")\n",
    "                print(f\"  Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Inference completed!\")\n",
    "    print(f\"Successfully processed: {len(all_results)}/{len(dataloader)} batches\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 保存汇总信息\n",
    "    if save_results and len(all_results) > 0:\n",
    "        summary_file = os.path.join(output_dir, 'inference_summary.json')\n",
    "        summary = [{\n",
    "            'batch_idx': r['batch_idx'],\n",
    "            'file_name': r['file_name']\n",
    "        } for r in all_results]\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        print(f\"Summary saved to {summary_file}\")\n",
    "        print(f\"Results saved to {output_dir}/\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ==================== 配置参数 ====================\n",
    "    data_dir = 'diffusion_planner_devset'  # 数据目录\n",
    "    model_path = 'model.pth'              # 模型路径\n",
    "    args_path = 'args.json'               # 参数配置文件（可选）\n",
    "    batch_size = 1                        # CPU推理建议batch_size=1\n",
    "    num_workers = 0                       # CPU模式建议设为0\n",
    "    device = 'cpu'\n",
    "    output_dir = 'inference_results'\n",
    "    use_ema = True                        # 是否使用EMA模型\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Diffusion Planner Inference Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    # 加载参数（可选）\n",
    "    if os.path.exists(args_path):\n",
    "        print(f\"\\n[1/4] Loading arguments from {args_path}...\")\n",
    "        args = load_args(args_path)\n",
    "        print(f\"  Loaded {len(vars(args))} parameters\")\n",
    "    else:\n",
    "        print(f\"\\n[1/4] Arguments file not found, skipping...\")\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    print(f\"\\n[2/4] Creating dataset from {data_dir}...\")\n",
    "    inference_dataset = InferenceDataset(data_dir)\n",
    "    \n",
    "    inference_loader = DataLoader(\n",
    "        inference_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    print(f\"  Dataset size: {len(inference_dataset)} samples\")\n",
    "    print(f\"  Number of batches: {len(inference_loader)}\")\n",
    "    \n",
    "    # 查看一个样本的数据结构\n",
    "    print(f\"\\n  Sample data structure:\")\n",
    "    sample = inference_dataset[0]\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"    {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            print(f\"    {key}: shape={value.shape}, dtype={value.dtype} (numpy)\")\n",
    "        else:\n",
    "            print(f\"    {key}: type={type(value)}, value={value if not hasattr(value, '__len__') or len(str(value)) < 50 else str(value)[:50]+'...'}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    print(f\"\\n[3/4] Loading model from {model_path}...\")\n",
    "    model = load_model(model_path, device=device, use_ema=use_ema)\n",
    "    \n",
    "    # 执行推理\n",
    "    print(f\"\\n[4/4] Running inference...\")\n",
    "    results = inference(\n",
    "        model, \n",
    "        inference_loader, \n",
    "        device=device,\n",
    "        save_results=True,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    if results and len(results) > 0:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        print(f\"Total samples processed: {len(results)}\")\n",
    "        print(f\"Results saved to: {output_dir}/\")\n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        print(\"\\nInference failed or no results generated.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "578bfddf-b4ff-4455-b16f-97ffcf82c20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "Keys: dict_keys(['model', 'ema_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "# 单独运行这段代码查看模型文件结构\n",
    "import torch\n",
    "checkpoint = torch.load('model.pth', map_location='cpu')\n",
    "print(type(checkpoint))\n",
    "if isinstance(checkpoint, dict):\n",
    "    print(\"Keys:\", checkpoint.keys())\n",
    "else:\n",
    "    print(\"Model type:\", type(checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61eb294a-8b75-47a7-a3a1-54add7d56107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Diffusion Planner Inference Pipeline\n",
      "============================================================\n",
      "\n",
      "[1/5] Loading arguments from args.json...\n",
      "  Loaded 26 parameters\n",
      "\n",
      "[2/5] Inspecting checkpoint...\n",
      "\n",
      "============================================================\n",
      "Checkpoint Structure:\n",
      "============================================================\n",
      "\n",
      "Key: 'model'\n",
      "  Type: <class 'collections.OrderedDict'>\n",
      "  Dict keys: ['module.encoder.encoder.neighbor_encoder.type_emb.weight', 'module.encoder.encoder.neighbor_encoder.type_emb.bias', 'module.encoder.encoder.neighbor_encoder.channel_pre_project.fc1.weight', 'module.encoder.encoder.neighbor_encoder.channel_pre_project.fc1.bias', 'module.encoder.encoder.neighbor_encoder.channel_pre_project.fc2.weight', 'module.encoder.encoder.neighbor_encoder.channel_pre_project.fc2.bias', 'module.encoder.encoder.neighbor_encoder.token_pre_project.fc1.weight', 'module.encoder.encoder.neighbor_encoder.token_pre_project.fc1.bias', 'module.encoder.encoder.neighbor_encoder.token_pre_project.fc2.weight', 'module.encoder.encoder.neighbor_encoder.token_pre_project.fc2.bias']...\n",
      "  First item type: <class 'torch.Tensor'>\n",
      "  First item shape: torch.Size([128, 3])\n",
      "\n",
      "Key: 'ema_state_dict'\n",
      "  Type: <class 'collections.OrderedDict'>\n",
      "  Dict keys: ['module.encoder.encoder.neighbor_encoder.type_emb.weight', 'module.encoder.encoder.neighbor_encoder.type_emb.bias', 'module.encoder.encoder.neighbor_encoder.channel_pre_project.fc1.weight', 'module.encoder.encoder.neighbor_encoder.channel_pre_project.fc1.bias', 'module.encoder.encoder.neighbor_encoder.channel_pre_project.fc2.weight', 'module.encoder.encoder.neighbor_encoder.channel_pre_project.fc2.bias', 'module.encoder.encoder.neighbor_encoder.token_pre_project.fc1.weight', 'module.encoder.encoder.neighbor_encoder.token_pre_project.fc1.bias', 'module.encoder.encoder.neighbor_encoder.token_pre_project.fc2.weight', 'module.encoder.encoder.neighbor_encoder.token_pre_project.fc2.bias']...\n",
      "  First item type: <class 'torch.Tensor'>\n",
      "  First item shape: torch.Size([128, 3])\n",
      "============================================================\n",
      "\n",
      "\n",
      "[3/5] Creating dataset from diffusion_planner_devset...\n",
      "Found 21 data files\n",
      "  Dataset size: 21 samples\n",
      "  Number of batches: 21\n",
      "\n",
      "  Sample data structure:\n",
      "    map_name: shape=(), dtype=<U12 (numpy)\n",
      "    token: shape=(), dtype=<U16 (numpy)\n",
      "    ego_current_state: shape=torch.Size([10]), dtype=torch.float32\n",
      "    ego_agent_future: shape=torch.Size([80, 3]), dtype=torch.float32\n",
      "    neighbor_agents_past: shape=torch.Size([32, 21, 11]), dtype=torch.float32\n",
      "    neighbor_agents_future: shape=torch.Size([32, 80, 3]), dtype=torch.float32\n",
      "    static_objects: shape=torch.Size([5, 10]), dtype=torch.float32\n",
      "    lanes: shape=torch.Size([70, 20, 12]), dtype=torch.float32\n",
      "    lanes_speed_limit: shape=torch.Size([70, 1]), dtype=torch.float32\n",
      "    lanes_has_speed_limit: shape=torch.Size([70, 1]), dtype=torch.bool\n",
      "    route_lanes: shape=torch.Size([25, 20, 12]), dtype=torch.float32\n",
      "    route_lanes_speed_limit: shape=torch.Size([25, 1]), dtype=torch.float32\n",
      "    route_lanes_has_speed_limit: shape=torch.Size([25, 1]), dtype=torch.bool\n",
      "\n",
      "[4/5] Loading model from checkpoint...\n",
      "\n",
      "Attempting to rebuild model from args...\n",
      "Args attributes: ['future_len', 'time_len', 'agent_state_dim', 'agent_num', 'static_objects_state_dim', 'static_objects_num', 'lane_len', 'lane_state_dim', 'lane_num', 'map_len', 'map_state_dim', 'map_num', 'route_len', 'route_state_dim', 'route_num', 'encoder_drop_path_rate', 'decoder_drop_path_rate', 'device', 'encoder_depth', 'decoder_depth', 'num_heads', 'hidden_dim', 'diffusion_model_type', 'predicted_neighbor_num', 'state_normalizer', 'observation_normalizer']\n",
      "Cannot import Diffusion_Planner, trying alternative approaches...\n",
      "\n",
      "Cannot import model class. Please ensure diffusion_planner.py is in the same directory or provide the model definition.\n",
      "\n",
      "Please ensure you have the model definition file (diffusion_planner.py)\n",
      "Or provide the path to import the Diffusion_Planner class\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "\n",
    "# 自定义数据集类\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        加载预处理好的数据\n",
    "        Args:\n",
    "            data_dir: 包含.npz文件的目录路径\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_files = sorted(list(self.data_dir.glob('*.npz')))\n",
    "        \n",
    "        if len(self.data_files) == 0:\n",
    "            raise ValueError(f\"No .npz files found in {data_dir}\")\n",
    "        \n",
    "        print(f\"Found {len(self.data_files)} data files\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        加载单个.npz文件\n",
    "        返回的数据应该包含模型需要的所有输入\n",
    "        \"\"\"\n",
    "        data_file = self.data_files[idx]\n",
    "        data = np.load(data_file, allow_pickle=True)\n",
    "        \n",
    "        # 将numpy数组转换为torch tensor，但保留字符串等特殊类型\n",
    "        sample = {}\n",
    "        for key in data.files:\n",
    "            value = data[key]\n",
    "            \n",
    "            # 检查数据类型\n",
    "            if isinstance(value, np.ndarray):\n",
    "                # 如果是数值类型，转换为tensor\n",
    "                if np.issubdtype(value.dtype, np.number):\n",
    "                    sample[key] = torch.from_numpy(value).float()\n",
    "                # 如果是布尔类型\n",
    "                elif np.issubdtype(value.dtype, np.bool_):\n",
    "                    sample[key] = torch.from_numpy(value)\n",
    "                # 如果是字符串或对象类型，保持原样\n",
    "                else:\n",
    "                    sample[key] = value\n",
    "            else:\n",
    "                sample[key] = value\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "def load_args(args_path='args.json'):\n",
    "    \"\"\"加载参数配置文件\"\"\"\n",
    "    with open(args_path, 'r') as f:\n",
    "        args_dict = json.load(f)\n",
    "    \n",
    "    # 将字典转换为argparse.Namespace对象\n",
    "    args = argparse.Namespace(**args_dict)\n",
    "    return args\n",
    "\n",
    "\n",
    "def inspect_checkpoint(model_path):\n",
    "    \"\"\"检查checkpoint的详细结构\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Checkpoint Structure:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for key in checkpoint.keys():\n",
    "        value = checkpoint[key]\n",
    "        print(f\"\\nKey: '{key}'\")\n",
    "        print(f\"  Type: {type(value)}\")\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            print(f\"  Dict keys: {list(value.keys())[:10]}...\")\n",
    "            if len(value) > 0:\n",
    "                first_key = list(value.keys())[0]\n",
    "                print(f\"  First item type: {type(value[first_key])}\")\n",
    "                if hasattr(value[first_key], 'shape'):\n",
    "                    print(f\"  First item shape: {value[first_key].shape}\")\n",
    "        elif hasattr(value, 'shape'):\n",
    "            print(f\"  Shape: {value.shape}\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def load_model_from_checkpoint(checkpoint, args, device='cpu', use_ema=True):\n",
    "    \"\"\"\n",
    "    从checkpoint重建模型\n",
    "    这需要知道模型的结构\n",
    "    \"\"\"\n",
    "    # 检查checkpoint中是否有模型结构信息\n",
    "    if 'model' in checkpoint and isinstance(checkpoint['model'], dict):\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        raise ValueError(\"Cannot find model state_dict in checkpoint\")\n",
    "    \n",
    "    # 从args中获取模型参数，尝试重建模型\n",
    "    # 这里需要您提供模型的定义或者从其他地方导入\n",
    "    print(\"\\nAttempting to rebuild model from args...\")\n",
    "    print(f\"Args attributes: {list(vars(args).keys())}\")\n",
    "    \n",
    "    # 尝试从训练代码中找到模型定义\n",
    "    # 如果您有原始的训练代码，需要导入Diffusion_Planner类\n",
    "    try:\n",
    "        # 尝试导入模型（需要根据您的项目结构调整）\n",
    "        from diffusion_planner import Diffusion_Planner\n",
    "        model = Diffusion_Planner(args)\n",
    "    except ImportError:\n",
    "        print(\"Cannot import Diffusion_Planner, trying alternative approaches...\")\n",
    "        \n",
    "        # 如果无法导入，我们可以尝试从state_dict推断模型结构\n",
    "        # 但这通常很困难\n",
    "        raise ImportError(\n",
    "            \"Cannot import model class. Please ensure diffusion_planner.py is in the same directory \"\n",
    "            \"or provide the model definition.\"\n",
    "        )\n",
    "    \n",
    "    # 加载state_dict\n",
    "    if use_ema and 'ema_state_dict' in checkpoint:\n",
    "        print(\"Loading EMA state dict...\")\n",
    "        model.load_state_dict(checkpoint['ema_state_dict'])\n",
    "    else:\n",
    "        print(\"Loading model state dict...\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully\")\n",
    "    print(f\"Model type: {type(model)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_batch_for_model(batch, device):\n",
    "    \"\"\"\n",
    "    准备batch数据用于模型输入\n",
    "    只将tensor类型的数据移到device，保留其他类型\n",
    "    \"\"\"\n",
    "    batch_device = {}\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            batch_device[key] = value.to(device)\n",
    "        else:\n",
    "            batch_device[key] = value\n",
    "    return batch_device\n",
    "\n",
    "\n",
    "def inference(model, dataloader, device='cpu', save_results=True, output_dir='inference_results'):\n",
    "    \"\"\"\n",
    "    执行推理\n",
    "    Args:\n",
    "        model: 加载的模型\n",
    "        dataloader: 数据加载器\n",
    "        device: 运行设备\n",
    "        save_results: 是否保存结果\n",
    "        output_dir: 结果保存目录\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if save_results:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(\"\\nStarting inference...\")\n",
    "    print(f\"Model type: {type(model).__name__}\")\n",
    "    available_methods = [m for m in dir(model) if not m.startswith('_') and callable(getattr(model, m))]\n",
    "    print(f\"Available methods: {available_methods[:20]}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # 将数据移到指定设备（只移动tensor）\n",
    "            batch_device = prepare_batch_for_model(batch, device)\n",
    "            \n",
    "            try:\n",
    "                # 执行推理 - 根据第一张图的代码，diffusion_planner可能有sample方法\n",
    "                if hasattr(model, 'conditional_sample'):\n",
    "                    predictions = model.conditional_sample(batch_device)\n",
    "                elif hasattr(model, 'sample'):\n",
    "                    predictions = model.sample(batch_device)\n",
    "                elif hasattr(model, 'predict'):\n",
    "                    predictions = model.predict(batch_device)\n",
    "                elif hasattr(model, 'forward'):\n",
    "                    predictions = model(batch_device)\n",
    "                else:\n",
    "                    raise AttributeError(f\"Model has no recognized inference method\")\n",
    "                \n",
    "                # 处理预测结果\n",
    "                if isinstance(predictions, torch.Tensor):\n",
    "                    result = predictions.cpu().numpy()\n",
    "                elif isinstance(predictions, dict):\n",
    "                    result = {k: v.cpu().numpy() if isinstance(v, torch.Tensor) else v \n",
    "                             for k, v in predictions.items()}\n",
    "                elif isinstance(predictions, (list, tuple)):\n",
    "                    result = [v.cpu().numpy() if isinstance(v, torch.Tensor) else v \n",
    "                             for v in predictions]\n",
    "                else:\n",
    "                    result = predictions\n",
    "                \n",
    "                # 记录结果\n",
    "                result_entry = {\n",
    "                    'batch_idx': batch_idx,\n",
    "                    'file_name': dataloader.dataset.data_files[batch_idx].name,\n",
    "                    'predictions': result\n",
    "                }\n",
    "                all_results.append(result_entry)\n",
    "                \n",
    "                # 保存单个批次结果\n",
    "                if save_results:\n",
    "                    output_file = os.path.join(output_dir, \n",
    "                                              f'result_{dataloader.dataset.data_files[batch_idx].stem}.npz')\n",
    "                    if isinstance(result, dict):\n",
    "                        np.savez(output_file, **result)\n",
    "                    elif isinstance(result, (list, tuple)):\n",
    "                        np.savez(output_file, *result)\n",
    "                    else:\n",
    "                        np.savez(output_file, predictions=result)\n",
    "                \n",
    "                if (batch_idx + 1) % 5 == 0:\n",
    "                    print(f\"Processed {batch_idx + 1}/{len(dataloader)} batches\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing batch {batch_idx} (file: {dataloader.dataset.data_files[batch_idx].name}):\")\n",
    "                print(f\"  Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Inference completed!\")\n",
    "    print(f\"Successfully processed: {len(all_results)}/{len(dataloader)} batches\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 保存汇总信息\n",
    "    if save_results and len(all_results) > 0:\n",
    "        summary_file = os.path.join(output_dir, 'inference_summary.json')\n",
    "        summary = [{\n",
    "            'batch_idx': r['batch_idx'],\n",
    "            'file_name': r['file_name']\n",
    "        } for r in all_results]\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        print(f\"Summary saved to {summary_file}\")\n",
    "        print(f\"Results saved to {output_dir}/\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ==================== 配置参数 ====================\n",
    "    data_dir = 'diffusion_planner_devset'  # 数据目录\n",
    "    model_path = 'model.pth'              # 模型路径\n",
    "    args_path = 'args.json'               # 参数配置文件（可选）\n",
    "    batch_size = 1                        # CPU推理建议batch_size=1\n",
    "    num_workers = 0                       # CPU模式建议设为0\n",
    "    device = 'cpu'\n",
    "    output_dir = 'inference_results'\n",
    "    use_ema = True                        # 是否使用EMA模型\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Diffusion Planner Inference Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    # 加载参数\n",
    "    if os.path.exists(args_path):\n",
    "        print(f\"\\n[1/5] Loading arguments from {args_path}...\")\n",
    "        args = load_args(args_path)\n",
    "        print(f\"  Loaded {len(vars(args))} parameters\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Arguments file not found: {args_path}\")\n",
    "    \n",
    "    # 检查checkpoint结构\n",
    "    print(f\"\\n[2/5] Inspecting checkpoint...\")\n",
    "    checkpoint = inspect_checkpoint(model_path)\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    print(f\"\\n[3/5] Creating dataset from {data_dir}...\")\n",
    "    inference_dataset = InferenceDataset(data_dir)\n",
    "    \n",
    "    inference_loader = DataLoader(\n",
    "        inference_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    print(f\"  Dataset size: {len(inference_dataset)} samples\")\n",
    "    print(f\"  Number of batches: {len(inference_loader)}\")\n",
    "    \n",
    "    # 查看一个样本的数据结构\n",
    "    print(f\"\\n  Sample data structure:\")\n",
    "    sample = inference_dataset[0]\n",
    "    for key, value in sample.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"    {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            print(f\"    {key}: shape={value.shape}, dtype={value.dtype} (numpy)\")\n",
    "        else:\n",
    "            print(f\"    {key}: type={type(value)}\")\n",
    "    \n",
    "    # 加载模型\n",
    "    print(f\"\\n[4/5] Loading model from checkpoint...\")\n",
    "    try:\n",
    "        model = load_model_from_checkpoint(checkpoint, args, device=device, use_ema=use_ema)\n",
    "    except ImportError as e:\n",
    "        print(f\"\\n{e}\")\n",
    "        print(\"\\nPlease ensure you have the model definition file (diffusion_planner.py)\")\n",
    "        print(\"Or provide the path to import the Diffusion_Planner class\")\n",
    "        return None\n",
    "    \n",
    "    # 执行推理\n",
    "    print(f\"\\n[5/5] Running inference...\")\n",
    "    results = inference(\n",
    "        model, \n",
    "        inference_loader, \n",
    "        device=device,\n",
    "        save_results=True,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    if results and len(results) > 0:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        print(f\"Total samples processed: {len(results)}\")\n",
    "        print(f\"Results saved to: {output_dir}/\")\n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        print(\"\\nInference failed or no results generated.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0661d-1be6-46e2-a649-6952e8c4bdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuplan_py38",
   "language": "python",
   "name": "nuplan_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
